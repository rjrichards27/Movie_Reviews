{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I always wrote this series off as being a comp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie was so poorly written and directed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The most interesting thing about Miryang (Secr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when i first read about \"berlin am meer\" i did...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>This is the kind of picture John Lassiter woul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>A MUST SEE! I saw WHIPPED at a press screening...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>NBC should be ashamed. I wouldn't allow my chi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>This movie is a clumsy mishmash of various gho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Formula movie about the illegitimate son of a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     I always wrote this series off as being a comp...      0\n",
       "1     1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...      0\n",
       "2     This movie was so poorly written and directed ...      0\n",
       "3     The most interesting thing about Miryang (Secr...      1\n",
       "4     when i first read about \"berlin am meer\" i did...      0\n",
       "...                                                 ...    ...\n",
       "4995  This is the kind of picture John Lassiter woul...      1\n",
       "4996  A MUST SEE! I saw WHIPPED at a press screening...      1\n",
       "4997  NBC should be ashamed. I wouldn't allow my chi...      0\n",
       "4998  This movie is a clumsy mishmash of various gho...      0\n",
       "4999  Formula movie about the illegitimate son of a ...      0\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"~/Desktop/ece684/movies.csv\")\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I always wrote this series off as being a comp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie was so poorly written and directed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when i first read about \"berlin am meer\" i did...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I saw a screening of this movie last night. I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4988</th>\n",
       "      <td>I had high expectations following \"My Beautifu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>This one is just like the 6th movie. The movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>NBC should be ashamed. I wouldn't allow my chi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>This movie is a clumsy mishmash of various gho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Formula movie about the illegitimate son of a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2495 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     I always wrote this series off as being a comp...      0\n",
       "1     1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...      0\n",
       "2     This movie was so poorly written and directed ...      0\n",
       "4     when i first read about \"berlin am meer\" i did...      0\n",
       "6     I saw a screening of this movie last night. I ...      0\n",
       "...                                                 ...    ...\n",
       "4988  I had high expectations following \"My Beautifu...      0\n",
       "4992  This one is just like the 6th movie. The movie...      0\n",
       "4997  NBC should be ashamed. I wouldn't allow my chi...      0\n",
       "4998  This movie is a clumsy mishmash of various gho...      0\n",
       "4999  Formula movie about the illegitimate son of a ...      0\n",
       "\n",
       "[2495 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = reviews[reviews[\"label\"]==0]\n",
    "subset\n",
    "#about half are negative and half are positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.isnull().values.any()\n",
    "# The data set has no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I saw a screening of this movie last night. I had high expectations going into it, but was definitely disappointed. Within 5 minutes of the opening, Williams is already campaigning for his presidency. And he becomes president in the first 40 minutes. So there goes all that aspect of the movie. The first half hour are hilarious. Don't get me wrong, the movie has its moments. But after the first half hour, it takes a turn for the worst. It becomes less of a comedy, and more of a thriller/drama/love story...which is pointless. the movie goes nowhere and stands still for a good 30 minutes. there are laughs interspersed here and there, but the consistently funny part is in the beginning and only the beginning. at one point, the biggest cheer i heard in the audience is when a person in the crowd yelled 'boooo' during a very confusingly emotional scene. Williams gives a great performance, right on par with his comedic style. Walken also delivers a strong supporting role as only he can. I think the one character that goes underrated is Lewis Black. Consistently vulgar and political, its funny to see him tone it down for a PG-13 rating. Overall, I would not pay to see the movie. Afterall, I saw it for free and even I was disappointed. The first half hour is solid, and its all downhill from there. Not really fitting into a category, the movie realizes half way through that it should not have been anything more than a one-hour comedy central special. 4.5/10\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[\"text\"][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Reformats Reviews\n",
    "\n",
    "\n",
    "def cleanReview(rev):\n",
    "    # Removing anything within a HTML tag\n",
    "    edited_rev = re.compile(r\"<[^>]+>\").sub(\" \", rev)\n",
    "    # Removing Punctuation\n",
    "    edited_rev = re.sub(r\"[^\\w\\s]\", \" \", edited_rev)\n",
    "    # Removing Numbers\n",
    "    edited_rev = re.sub(r\"[0-9]\", \" \", edited_rev)\n",
    "    # Removing single characters\n",
    "    edited_rev = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", edited_rev)\n",
    "    # Removing multiple spaces\n",
    "    edited_rev = re.sub(r\"\\s+\", \" \", edited_rev)\n",
    "\n",
    "    return edited_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"text\"] = reviews[\"text\"].apply(lambda x: x.replace(\"wouldn't\", \"would not\"))\n",
    "reviews[\"text\"] = reviews[\"text\"].apply(lambda x: x.replace(\"won't\", \"will not\"))\n",
    "reviews[\"text\"] = reviews[\"text\"].apply(lambda x: x.replace(\"can't\", \"can not\"))\n",
    "reviews[\"text\"] = reviews[\"text\"].apply(lambda x: x.replace(\"couldn't\", \"could not\"))\n",
    "reviews[\"text\"] = reviews[\"text\"].apply(lambda x: x.replace(\"I'm\", \"I am\"))\n",
    "reviews[\"text\"] = reviews[\"text\"].apply(lambda x: x.replace(\"ain't\", \"is not\"))\n",
    "reviews[\"text\"] = reviews[\"text\"].apply(lambda x: x.replace(\"shouldn't\", \"should not\"))\n",
    "reviews[\"text\"] = reviews[\"text\"].apply(lambda x: x.replace(\"(\\w+)'ll\", \"\\g<1> will\"))\n",
    "reviews[\"text\"] = reviews[\"text\"].apply(lambda x: x.replace(\"(\\w+)'ve\", \"\\g<1> have\"))\n",
    "reviews[\"text\"] = reviews[\"text\"].apply(lambda x: x.replace(\"(\\w+)'s\", \"\\g<1> is\"))\n",
    "reviews[\"text\"] = reviews[\"text\"].apply(lambda x: x.replace(\"(\\w+)'re\", \"\\g<1> are\"))\n",
    "reviews[\"text\"] = reviews[\"text\"].apply(lambda x: x.replace(\"(\\w+)'d\", \"\\g<1> would\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       I always wrote this series off as being a comp...\n",
       "1       1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...\n",
       "2       This movie was so poorly written and directed ...\n",
       "3       The most interesting thing about Miryang (Secr...\n",
       "4       when i first read about \"berlin am meer\" i did...\n",
       "                              ...                        \n",
       "4995    This is the kind of picture John Lassiter woul...\n",
       "4996    A MUST SEE! I saw WHIPPED at a press screening...\n",
       "4997    NBC should be ashamed. I would not allow my ch...\n",
       "4998    This movie is a clumsy mishmash of various gho...\n",
       "4999    Formula movie about the illegitimate son of a ...\n",
       "Name: text, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list = []\n",
    "\n",
    "# Adding all of the movie reviews to a list\n",
    "for r in reviews[\"text\"]:\n",
    "    review_list.append(cleanReview(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start of tokenizing\n",
    "words = \" \".join(review_list)\n",
    "words = list(words.split(\" \"))\n",
    "\n",
    "#Getting word counts\n",
    "totalCounts = {}\n",
    "for j in range(len(words)):\n",
    "    if words[j] not in totalCounts:\n",
    "        totalCounts[words[j]] = 1\n",
    "    if words[j] in totalCounts:\n",
    "        totalCounts[words[j]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting dictionary to have highest word counts first\n",
    "sorted_counts=dict(sorted(totalCounts.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the highest word count value be 1, etc.\n",
    "w_to_i = {w:i+1 for i, (w) in enumerate(sorted_counts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the words by replacing the word with its given integer\n",
    "X = []\n",
    "for m in review_list:\n",
    "    r = [w_to_i[w] for w in m.split()]\n",
    "    X.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221.3644"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "lengths = []\n",
    "for i in range(len(X)):\n",
    "    l = len(X[i])\n",
    "    lengths.append(l)\n",
    "statistics.mean(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding for different review lengths\n",
    "X_array = np.zeros((len(X), 221), dtype =int)\n",
    "for i, rev2 in enumerate(X):\n",
    "    rev_length = len(rev2)\n",
    "\n",
    "    if rev_length <= 221:\n",
    "        zeros = list(np.zeros(221-rev_length))\n",
    "        mat = zeros + rev2\n",
    "    elif rev_length > 221:\n",
    "        mat = rev2[0:221]\n",
    "    X_array[i,:] = np.array(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a training and test data sets\n",
    "y = reviews[\"label\"]\n",
    "\n",
    "y = np.array(list(map(lambda x: 1 if x == 1 else 0, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_array, y, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# create Tensor Dataset\n",
    "train_data = TensorDataset(torch.LongTensor(X_train), torch.LongTensor(y_train))\n",
    "valid_data=TensorDataset(torch.LongTensor(X_valid), torch.LongTensor(y_valid))\n",
    "test_data = TensorDataset(torch.LongTensor(X_test), torch.LongTensor(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader\n",
    "batch_size=50 #50\n",
    "train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentalLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_words, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True\n",
    "        )\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.4) #0.3\n",
    "\n",
    "        # Linear and sigmoid layer\n",
    "        self.fc1 = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size()\n",
    "\n",
    "        #   Embadding and LSTM output\n",
    "        embedd = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embedd, hidden)\n",
    "\n",
    "        # stack up the lstm output\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # dropout and fully connected layers\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout(out)\n",
    "        '''out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)'''\n",
    "        sig_out = self.sigmoid(out)\n",
    "\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize Hidden STATE\"\"\"\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentalLSTM(\n",
      "  (embedding): Embedding(1111828, 221)\n",
      "  (lstm): LSTM(221, 250, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc1): Linear(in_features=250, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "#vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
    "num_words = len(words)\n",
    "output_size = 1\n",
    "embedding_dim = 221\n",
    "hidden_dim = 250 #256 250\n",
    "n_layers = 2 #2\n",
    "\n",
    "net = SentimentalLSTM(num_words, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10... Step: 100... Loss: 0.696152... Val Loss: 0.691314\n",
      "Epoch: 4/10... Step: 200... Loss: 0.692318... Val Loss: 0.687720\n",
      "Epoch: 5/10... Step: 300... Loss: 0.661424... Val Loss: 0.672706\n",
      "Epoch: 7/10... Step: 400... Loss: 0.579090... Val Loss: 0.621010\n",
      "Epoch: 9/10... Step: 500... Loss: 0.614898... Val Loss: 0.600089\n",
      "Epoch: 10/10... Step: 600... Loss: 0.484117... Val Loss: 0.571037\n"
     ]
    }
   ],
   "source": [
    "lr=0.0001 #0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 10 # is approx where I noticed the validation loss stop decreasing, 3\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs=inputs.cuda()\n",
    "            labels=labels.cuda()\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "        \n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                inputs, labels = inputs, labels \n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.577\n",
      "Test accuracy: 0.690\n"
     ]
    }
   ],
   "source": [
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "\n",
    "    output, h = net(inputs, h)\n",
    "\n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.690\n"
     ]
    }
   ],
   "source": [
    "train_acc = num_correct/len(train_loader.dataset)\n",
    "print(\"Train accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Plot training progress.\"\"\"\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "ax[0].plot(test_losses)\n",
    "ax[0].set_ylabel(\"Test Loss\")\n",
    "ax[0].set_title(\"Test Loss\")\n",
    "\n",
    "ax[1].plot(test_acc)\n",
    "ax[1].set_ylabel(\"Classification Accuracy\")\n",
    "ax[1].set_title(\"Test Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "ax[0].plot(val_losses)\n",
    "ax[0].set_ylabel(\"Validation Loss\")\n",
    "ax[0].set_title(\"Validation Loss\")\n",
    "\n",
    "ax[1].plot(test_acc)\n",
    "ax[1].set_ylabel(\"Classification Accuracy\")\n",
    "ax[1].set_title(\"Test Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def predict(reviews, net):\\n    rev1_lst=[]\\n    rev1_lst.append(cleanReview(rev1))\\n    rev1_lst\\n\\n    X2 = []\\n    for m2 in rev1_lst:\\n        r2 = [w_to_i[w] for w in m2.split()]\\n        X2.append(r2)\\n\\n    X_array2 = np.zeros((len(X2), 221), dtype =int)\\n    rev_length2 = len(X2)\\n\\n    if rev_length2 <= 221:\\n        zeros2 = list(np.zeros(221-rev_length))\\n        mat2 = X2 + zeros2\\n    elif rev_length2 > 221:\\n        mat2 = rev2[0:221]\\n    X_array2 = np.array(mat2)\\n\\n    h = net.init_hidden(batch_size)\\n\\n\\n    # batch loop\\n    for inputs, labels in train_loader:\\n        counter += 1\\n\\n        if(train_on_gpu):\\n            inputs=inputs.cuda()\\n            labels=labels.cuda()\\n        # Creating new variables for the hidden state, otherwise\\n        # we'd backprop through the entire training history\\n        h = tuple([each.data for each in h])\\n\\n        # zero accumulated gradients\\n        net.zero_grad()\\n        \\n        # get the output from the model\\n        output, h = net(inputs, h)\\n\\n    return output.item()\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def predict(reviews, net):\n",
    "    rev1_lst=[]\n",
    "    rev1_lst.append(cleanReview(rev1))\n",
    "    rev1_lst\n",
    "\n",
    "    X2 = []\n",
    "    for m2 in rev1_lst:\n",
    "        r2 = [w_to_i[w] for w in m2.split()]\n",
    "        X2.append(r2)\n",
    "\n",
    "    X_array2 = np.zeros((len(X2), 221), dtype =int)\n",
    "    rev_length2 = len(X2)\n",
    "\n",
    "    if rev_length2 <= 221:\n",
    "        zeros2 = list(np.zeros(221-rev_length))\n",
    "        mat2 = X2 + zeros2\n",
    "    elif rev_length2 > 221:\n",
    "        mat2 = rev2[0:221]\n",
    "    X_array2 = np.array(mat2)\n",
    "\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs=inputs.cuda()\n",
    "            labels=labels.cuda()\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "        \n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "    return output.item()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(reviews):\n",
    "    review_list2 = []\n",
    "\n",
    "    # Adding all of the movie reviews to a list\n",
    "    for r2 in reviews[\"text\"]:\n",
    "        review_list2.append(cleanReview(r2))\n",
    "\n",
    "    # Encoding the words by replacing the word with its given integer\n",
    "    X2 = []\n",
    "    for m2 in review_list2:\n",
    "        r2 = [w_to_i[w2] for w2 in m2.split()]\n",
    "        X2.append(r)\n",
    "\n",
    "    #Padding for different review lengths\n",
    "    X_array2 = np.zeros((len(X2), 221), dtype =int)\n",
    "    for i2, rev8 in enumerate(X2):\n",
    "        rev_length2 = len(rev8)\n",
    "\n",
    "        if rev_length2 <= 221:\n",
    "            zeros2 = list(np.zeros(221-rev_length2))\n",
    "            mat2 = zeros2 + rev8\n",
    "        elif rev_length2 > 221:\n",
    "            mat2 = rev8[0:221]\n",
    "        X_array2[i2,:] = np.array(mat2)\n",
    "\n",
    "    counter = 0\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    final_probs = []\n",
    "    with torch.no_grad():\n",
    "        # batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs=inputs.cuda()\n",
    "                labels=labels.cuda()\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "        \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            final_probs.append(output)\n",
    "\n",
    "        return final_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.1833, 0.7122, 0.6963, 0.7764, 0.4810, 0.7806, 0.7723, 0.2054, 0.8332,\n",
      "        0.1642, 0.7441, 0.2583, 0.2985, 0.2478, 0.1306, 0.1607, 0.8262, 0.1646,\n",
      "        0.6686, 0.3993, 0.1189, 0.1562, 0.4004, 0.2801, 0.8193, 0.2178, 0.8933,\n",
      "        0.3521, 0.2682, 0.8213, 0.2880, 0.7835, 0.6741, 0.7987, 0.9099, 0.1131,\n",
      "        0.7110, 0.3359, 0.8120, 0.8211, 0.5013, 0.8275, 0.5388, 0.8589, 0.8655,\n",
      "        0.1233, 0.8406, 0.1916, 0.8341, 0.5186]), tensor([0.8223, 0.1499, 0.5615, 0.6986, 0.5422, 0.8505, 0.6626, 0.8429, 0.1389,\n",
      "        0.2150, 0.8184, 0.5047, 0.8741, 0.6570, 0.6241, 0.1812, 0.8994, 0.1934,\n",
      "        0.6604, 0.6460, 0.7666, 0.6805, 0.5312, 0.7316, 0.8447, 0.8233, 0.8060,\n",
      "        0.8306, 0.3077, 0.1358, 0.2183, 0.1828, 0.2796, 0.8595, 0.1610, 0.4470,\n",
      "        0.2911, 0.8276, 0.7262, 0.1439, 0.3001, 0.8865, 0.3945, 0.5489, 0.3301,\n",
      "        0.1960, 0.7462, 0.8535, 0.8358, 0.3651]), tensor([0.8845, 0.1149, 0.1112, 0.4812, 0.1107, 0.1877, 0.8932, 0.4947, 0.2166,\n",
      "        0.8265, 0.6411, 0.6938, 0.7177, 0.7263, 0.8135, 0.5592, 0.1451, 0.6530,\n",
      "        0.6302, 0.5780, 0.6696, 0.8461, 0.2826, 0.2597, 0.4421, 0.4630, 0.8046,\n",
      "        0.1078, 0.2867, 0.4474, 0.7112, 0.7428, 0.4942, 0.2141, 0.6653, 0.1128,\n",
      "        0.8235, 0.7993, 0.2254, 0.5810, 0.6218, 0.3783, 0.7916, 0.8651, 0.6627,\n",
      "        0.4697, 0.4007, 0.7772, 0.2569, 0.8446]), tensor([0.2201, 0.8892, 0.2341, 0.8075, 0.1262, 0.7667, 0.3919, 0.1531, 0.3734,\n",
      "        0.1520, 0.8273, 0.4616, 0.7977, 0.8695, 0.1596, 0.7758, 0.8810, 0.7918,\n",
      "        0.7702, 0.2417, 0.6808, 0.8255, 0.2662, 0.1593, 0.8140, 0.7436, 0.7225,\n",
      "        0.1329, 0.6294, 0.8251, 0.9364, 0.6363, 0.8481, 0.7803, 0.5401, 0.3210,\n",
      "        0.7101, 0.6502, 0.8276, 0.6208, 0.4813, 0.1610, 0.1677, 0.1951, 0.2813,\n",
      "        0.7734, 0.2564, 0.1611, 0.7717, 0.4447]), tensor([0.2275, 0.5836, 0.1853, 0.8079, 0.7321, 0.6480, 0.8151, 0.1930, 0.5329,\n",
      "        0.5452, 0.3178, 0.8382, 0.7445, 0.8315, 0.3178, 0.1313, 0.3903, 0.6875,\n",
      "        0.2083, 0.2325, 0.6829, 0.8843, 0.7747, 0.1700, 0.6154, 0.8204, 0.6448,\n",
      "        0.1363, 0.8328, 0.7219, 0.3719, 0.6935, 0.7876, 0.3032, 0.8897, 0.3548,\n",
      "        0.7749, 0.6620, 0.8478, 0.8496, 0.2269, 0.6996, 0.8718, 0.7872, 0.7823,\n",
      "        0.3790, 0.3548, 0.1770, 0.8779, 0.4814]), tensor([0.4773, 0.5239, 0.6673, 0.2104, 0.3608, 0.3062, 0.5974, 0.7214, 0.7906,\n",
      "        0.2646, 0.2637, 0.8671, 0.8087, 0.4005, 0.3824, 0.7196, 0.7078, 0.6716,\n",
      "        0.1120, 0.7748, 0.8764, 0.1399, 0.7316, 0.2450, 0.2794, 0.7836, 0.2791,\n",
      "        0.2443, 0.1329, 0.7959, 0.8889, 0.6032, 0.6026, 0.7816, 0.2849, 0.8100,\n",
      "        0.8453, 0.3094, 0.8711, 0.8607, 0.2175, 0.8787, 0.2913, 0.4870, 0.1838,\n",
      "        0.2619, 0.1000, 0.7907, 0.6368, 0.8465]), tensor([0.6932, 0.8303, 0.8025, 0.2619, 0.1764, 0.3997, 0.8832, 0.1446, 0.6819,\n",
      "        0.8512, 0.1826, 0.2993, 0.1940, 0.1257, 0.8462, 0.2187, 0.5401, 0.3822,\n",
      "        0.1622, 0.4207, 0.1339, 0.1642, 0.7022, 0.1961, 0.2488, 0.1407, 0.7824,\n",
      "        0.7621, 0.2862, 0.2626, 0.7583, 0.2898, 0.7832, 0.3859, 0.4231, 0.3872,\n",
      "        0.1980, 0.2611, 0.6741, 0.8733, 0.7658, 0.7852, 0.6708, 0.8176, 0.1749,\n",
      "        0.1336, 0.6374, 0.6232, 0.2746, 0.7471]), tensor([0.2422, 0.1237, 0.3092, 0.6293, 0.2894, 0.2999, 0.2505, 0.7631, 0.1314,\n",
      "        0.0783, 0.2686, 0.8779, 0.3874, 0.5425, 0.7521, 0.6981, 0.2001, 0.8533,\n",
      "        0.1317, 0.7403, 0.6977, 0.7362, 0.3375, 0.2311, 0.6271, 0.5555, 0.3886,\n",
      "        0.8174, 0.6459, 0.7498, 0.1508, 0.6958, 0.8654, 0.6518, 0.5550, 0.8155,\n",
      "        0.1253, 0.7216, 0.1511, 0.6691, 0.8460, 0.8816, 0.7232, 0.4046, 0.4070,\n",
      "        0.1174, 0.3367, 0.3787, 0.2885, 0.8888]), tensor([0.7514, 0.1791, 0.2435, 0.3340, 0.5564, 0.8395, 0.7218, 0.7774, 0.2141,\n",
      "        0.2886, 0.1668, 0.2873, 0.8598, 0.7763, 0.7688, 0.8421, 0.1168, 0.8344,\n",
      "        0.3081, 0.8361, 0.7897, 0.1689, 0.4914, 0.7602, 0.7231, 0.8662, 0.5064,\n",
      "        0.1277, 0.2236, 0.7806, 0.6894, 0.7426, 0.7174, 0.2802, 0.8756, 0.5210,\n",
      "        0.2642, 0.6953, 0.1433, 0.8400, 0.4303, 0.5560, 0.7926, 0.7804, 0.1548,\n",
      "        0.2110, 0.7609, 0.2305, 0.8725, 0.6593]), tensor([0.2815, 0.2399, 0.8205, 0.2681, 0.2008, 0.8833, 0.7497, 0.8238, 0.6319,\n",
      "        0.6682, 0.4952, 0.2304, 0.3293, 0.1261, 0.1898, 0.2339, 0.6935, 0.5609,\n",
      "        0.3148, 0.1813, 0.4667, 0.7807, 0.2276, 0.3364, 0.5828, 0.5928, 0.6968,\n",
      "        0.1730, 0.7900, 0.7131, 0.8491, 0.8337, 0.8728, 0.1541, 0.6451, 0.2868,\n",
      "        0.8406, 0.6476, 0.2120, 0.3637, 0.6625, 0.7460, 0.5998, 0.5296, 0.8138,\n",
      "        0.2188, 0.6852, 0.1713, 0.6766, 0.2220]), tensor([0.5384, 0.9046, 0.4853, 0.6625, 0.1763, 0.2789, 0.1180, 0.8843, 0.6084,\n",
      "        0.4174, 0.1319, 0.7619, 0.2193, 0.4044, 0.1558, 0.5328, 0.8544, 0.2250,\n",
      "        0.4756, 0.7809, 0.7696, 0.6048, 0.4527, 0.6106, 0.2749, 0.7869, 0.8871,\n",
      "        0.4051, 0.1745, 0.3402, 0.8264, 0.1526, 0.8371, 0.8428, 0.3208, 0.8470,\n",
      "        0.7516, 0.8211, 0.5403, 0.8787, 0.7493, 0.8315, 0.8411, 0.6306, 0.7690,\n",
      "        0.3235, 0.2715, 0.2225, 0.2349, 0.8581]), tensor([0.1975, 0.2059, 0.3056, 0.2889, 0.7819, 0.4147, 0.1773, 0.8504, 0.6625,\n",
      "        0.2138, 0.7423, 0.6146, 0.1467, 0.2820, 0.8006, 0.8708, 0.7788, 0.5575,\n",
      "        0.4722, 0.1462, 0.8444, 0.6563, 0.7842, 0.4055, 0.8497, 0.7883, 0.5031,\n",
      "        0.8041, 0.7747, 0.1917, 0.7898, 0.8260, 0.7231, 0.3513, 0.6820, 0.1576,\n",
      "        0.3111, 0.7796, 0.8407, 0.2905, 0.6619, 0.7245, 0.8204, 0.4841, 0.4249,\n",
      "        0.1809, 0.6453, 0.3919, 0.8142, 0.7621]), tensor([0.1596, 0.3911, 0.2175, 0.7763, 0.1581, 0.8939, 0.9129, 0.4477, 0.4235,\n",
      "        0.8253, 0.8233, 0.6860, 0.2185, 0.3961, 0.1695, 0.8709, 0.3942, 0.6583,\n",
      "        0.0950, 0.5353, 0.3915, 0.8582, 0.4655, 0.8093, 0.2420, 0.6881, 0.5893,\n",
      "        0.6722, 0.3089, 0.2562, 0.4659, 0.8580, 0.7888, 0.1052, 0.1776, 0.5353,\n",
      "        0.6423, 0.2693, 0.5879, 0.7295, 0.7838, 0.8248, 0.4335, 0.4692, 0.8287,\n",
      "        0.7139, 0.6117, 0.8542, 0.1314, 0.6747]), tensor([0.2369, 0.2967, 0.8165, 0.2815, 0.2277, 0.2193, 0.8931, 0.1959, 0.7886,\n",
      "        0.1639, 0.1510, 0.8159, 0.1793, 0.2189, 0.8477, 0.7713, 0.3497, 0.3839,\n",
      "        0.2179, 0.1679, 0.3014, 0.6862, 0.8560, 0.6190, 0.1733, 0.3336, 0.9086,\n",
      "        0.3348, 0.3058, 0.6633, 0.8072, 0.2239, 0.8826, 0.3982, 0.6348, 0.3161,\n",
      "        0.3816, 0.8658, 0.2293, 0.7348, 0.1860, 0.8092, 0.3857, 0.1689, 0.8688,\n",
      "        0.3252, 0.5498, 0.8310, 0.8834, 0.4508]), tensor([0.1247, 0.2451, 0.1169, 0.8633, 0.4462, 0.8628, 0.4935, 0.1434, 0.3181,\n",
      "        0.2615, 0.1540, 0.7454, 0.3790, 0.8351, 0.7127, 0.1898, 0.7833, 0.7824,\n",
      "        0.6551, 0.7577, 0.2517, 0.6247, 0.8523, 0.4094, 0.3519, 0.8455, 0.2169,\n",
      "        0.2228, 0.7772, 0.1737, 0.8327, 0.2674, 0.4990, 0.2401, 0.7199, 0.1636,\n",
      "        0.8132, 0.7215, 0.8461, 0.1612, 0.6641, 0.3669, 0.5991, 0.4342, 0.3069,\n",
      "        0.1993, 0.2490, 0.7202, 0.5562, 0.1288]), tensor([0.3391, 0.8419, 0.1551, 0.8683, 0.2268, 0.8800, 0.3548, 0.5553, 0.3610,\n",
      "        0.8860, 0.2937, 0.7957, 0.7379, 0.6353, 0.4688, 0.8366, 0.8284, 0.6290,\n",
      "        0.2647, 0.4009, 0.8363, 0.2827, 0.8455, 0.8038, 0.1970, 0.8746, 0.8217,\n",
      "        0.1621, 0.3771, 0.8135, 0.8652, 0.6553, 0.2288, 0.3110, 0.6645, 0.7413,\n",
      "        0.6998, 0.7861, 0.7806, 0.3248, 0.7423, 0.7748, 0.1236, 0.8694, 0.7762,\n",
      "        0.8695, 0.6714, 0.8496, 0.3420, 0.2946]), tensor([0.5315, 0.1286, 0.4116, 0.8474, 0.1548, 0.7373, 0.5214, 0.4612, 0.5633,\n",
      "        0.1847, 0.5490, 0.8611, 0.2846, 0.4384, 0.8750, 0.6588, 0.1920, 0.3265,\n",
      "        0.5890, 0.7763, 0.1484, 0.1366, 0.8512, 0.8765, 0.2891, 0.6235, 0.3316,\n",
      "        0.6925, 0.7565, 0.8865, 0.4184, 0.3185, 0.3994, 0.6532, 0.1786, 0.5841,\n",
      "        0.1101, 0.6966, 0.2048, 0.9024, 0.7777, 0.3354, 0.1972, 0.8674, 0.8590,\n",
      "        0.1940, 0.1932, 0.8744, 0.1127, 0.1736]), tensor([0.4963, 0.3075, 0.5967, 0.2067, 0.8413, 0.8568, 0.2148, 0.2450, 0.3174,\n",
      "        0.8117, 0.8557, 0.2417, 0.4003, 0.5669, 0.7553, 0.8427, 0.6792, 0.8211,\n",
      "        0.4420, 0.3192, 0.4183, 0.1462, 0.3467, 0.8203, 0.7557, 0.3915, 0.5649,\n",
      "        0.6857, 0.8010, 0.7181, 0.2058, 0.3409, 0.6730, 0.1688, 0.2559, 0.2105,\n",
      "        0.8942, 0.2095, 0.2341, 0.4711, 0.2620, 0.8231, 0.1949, 0.6237, 0.6288,\n",
      "        0.4698, 0.3540, 0.6188, 0.4324, 0.2348]), tensor([0.4039, 0.4625, 0.3578, 0.3385, 0.8707, 0.2723, 0.6403, 0.7710, 0.8195,\n",
      "        0.8439, 0.7678, 0.2262, 0.7394, 0.4554, 0.6052, 0.8382, 0.1561, 0.8450,\n",
      "        0.6219, 0.1475, 0.7027, 0.1452, 0.1085, 0.3130, 0.5088, 0.2776, 0.1236,\n",
      "        0.8241, 0.7541, 0.2225, 0.8212, 0.5037, 0.4872, 0.3105, 0.1958, 0.0974,\n",
      "        0.8606, 0.1637, 0.1619, 0.2803, 0.6354, 0.2719, 0.8546, 0.5214, 0.4678,\n",
      "        0.8889, 0.8901, 0.8376, 0.6121, 0.7782]), tensor([0.2318, 0.7666, 0.8591, 0.1925, 0.1349, 0.8582, 0.3419, 0.2517, 0.7848,\n",
      "        0.6445, 0.3360, 0.7443, 0.5739, 0.4025, 0.1825, 0.1624, 0.1693, 0.7625,\n",
      "        0.8221, 0.4113, 0.4276, 0.2122, 0.3302, 0.8502, 0.1578, 0.8266, 0.1315,\n",
      "        0.8602, 0.8533, 0.7446, 0.3175, 0.6712, 0.8707, 0.7201, 0.1925, 0.1613,\n",
      "        0.6765, 0.1979, 0.4452, 0.6166, 0.3711, 0.7672, 0.8375, 0.2814, 0.4444,\n",
      "        0.6429, 0.3141, 0.8111, 0.6745, 0.7344]), tensor([0.8969, 0.4347, 0.5585, 0.7666, 0.1108, 0.2061, 0.1221, 0.8100, 0.8081,\n",
      "        0.8438, 0.1406, 0.1771, 0.5014, 0.8680, 0.1780, 0.3163, 0.5550, 0.1964,\n",
      "        0.9001, 0.1392, 0.2084, 0.3206, 0.8368, 0.2327, 0.1754, 0.8496, 0.7763,\n",
      "        0.2285, 0.4275, 0.6209, 0.7539, 0.8144, 0.1450, 0.6622, 0.2246, 0.3291,\n",
      "        0.6323, 0.8353, 0.2463, 0.0985, 0.7184, 0.5862, 0.3989, 0.5460, 0.8326,\n",
      "        0.8203, 0.8489, 0.7938, 0.1902, 0.7845]), tensor([0.6101, 0.3454, 0.1750, 0.6361, 0.1325, 0.8742, 0.6971, 0.1231, 0.9092,\n",
      "        0.2967, 0.7177, 0.6153, 0.3569, 0.1994, 0.2148, 0.8770, 0.2145, 0.1417,\n",
      "        0.7582, 0.8054, 0.8570, 0.3929, 0.2547, 0.8260, 0.4022, 0.4193, 0.4747,\n",
      "        0.8748, 0.4374, 0.3922, 0.2111, 0.8170, 0.8053, 0.6635, 0.3123, 0.7720,\n",
      "        0.8528, 0.1948, 0.7384, 0.1515, 0.0995, 0.2446, 0.5871, 0.8309, 0.1566,\n",
      "        0.2267, 0.7995, 0.7304, 0.7632, 0.8482]), tensor([0.6126, 0.4314, 0.7864, 0.3718, 0.6963, 0.8335, 0.4736, 0.7000, 0.5528,\n",
      "        0.3833, 0.1371, 0.8333, 0.8587, 0.3013, 0.6208, 0.2701, 0.0900, 0.8160,\n",
      "        0.8523, 0.4504, 0.8680, 0.8377, 0.2584, 0.1636, 0.3553, 0.8304, 0.8287,\n",
      "        0.8720, 0.1968, 0.7579, 0.2185, 0.2200, 0.8813, 0.1486, 0.7083, 0.8038,\n",
      "        0.5391, 0.7614, 0.6998, 0.2495, 0.6346, 0.4640, 0.5234, 0.4905, 0.2212,\n",
      "        0.2640, 0.7708, 0.4605, 0.8694, 0.3803]), tensor([0.7505, 0.7227, 0.2414, 0.2048, 0.6875, 0.8619, 0.7854, 0.1145, 0.2253,\n",
      "        0.1692, 0.6395, 0.5240, 0.8570, 0.4215, 0.7759, 0.7580, 0.4977, 0.7140,\n",
      "        0.4335, 0.8351, 0.2990, 0.1996, 0.8524, 0.2321, 0.2838, 0.8372, 0.8929,\n",
      "        0.2789, 0.8536, 0.1992, 0.4023, 0.7685, 0.1503, 0.8551, 0.7963, 0.8652,\n",
      "        0.7767, 0.2863, 0.7614, 0.6651, 0.8766, 0.7913, 0.5800, 0.7452, 0.6087,\n",
      "        0.2517, 0.1527, 0.7069, 0.1047, 0.5095]), tensor([0.2538, 0.2070, 0.5426, 0.2489, 0.4237, 0.6196, 0.3727, 0.2851, 0.0820,\n",
      "        0.1585, 0.7337, 0.6976, 0.2068, 0.6841, 0.3004, 0.1613, 0.9035, 0.5863,\n",
      "        0.2378, 0.8094, 0.2004, 0.5480, 0.1680, 0.8463, 0.1519, 0.5905, 0.8401,\n",
      "        0.8103, 0.6942, 0.1188, 0.8290, 0.2844, 0.1969, 0.7533, 0.8398, 0.8044,\n",
      "        0.7876, 0.3517, 0.4756, 0.3031, 0.3572, 0.7265, 0.2640, 0.1942, 0.7594,\n",
      "        0.8185, 0.3893, 0.5203, 0.2874, 0.8084]), tensor([0.3393, 0.1905, 0.4378, 0.7779, 0.4411, 0.8212, 0.4770, 0.5650, 0.6927,\n",
      "        0.5907, 0.5091, 0.3344, 0.7637, 0.5595, 0.0973, 0.2519, 0.3045, 0.7099,\n",
      "        0.1282, 0.6505, 0.8223, 0.8154, 0.2681, 0.1720, 0.6342, 0.1493, 0.5853,\n",
      "        0.5549, 0.3446, 0.3679, 0.1814, 0.5255, 0.7880, 0.8636, 0.5112, 0.3000,\n",
      "        0.5643, 0.7369, 0.8045, 0.2215, 0.3471, 0.7507, 0.3211, 0.1729, 0.3542,\n",
      "        0.8113, 0.6727, 0.4587, 0.1384, 0.7788]), tensor([0.5004, 0.1466, 0.7395, 0.2089, 0.8833, 0.2939, 0.2831, 0.2187, 0.2830,\n",
      "        0.8197, 0.8804, 0.1960, 0.1012, 0.6180, 0.3391, 0.5747, 0.7609, 0.9084,\n",
      "        0.2955, 0.6182, 0.2407, 0.8756, 0.7923, 0.3339, 0.2326, 0.3889, 0.8733,\n",
      "        0.6516, 0.7887, 0.7762, 0.2041, 0.2127, 0.5499, 0.1960, 0.6359, 0.3405,\n",
      "        0.8184, 0.1890, 0.8163, 0.7429, 0.5480, 0.2774, 0.4173, 0.7660, 0.7498,\n",
      "        0.5210, 0.3386, 0.6529, 0.3112, 0.7416]), tensor([0.3463, 0.8181, 0.3511, 0.2759, 0.7920, 0.7277, 0.8554, 0.6074, 0.4632,\n",
      "        0.2749, 0.4573, 0.4971, 0.8269, 0.8336, 0.3253, 0.2397, 0.5975, 0.2109,\n",
      "        0.8362, 0.7709, 0.4467, 0.8104, 0.8172, 0.8173, 0.4565, 0.1241, 0.5251,\n",
      "        0.5811, 0.3152, 0.5673, 0.8832, 0.6753, 0.7749, 0.8253, 0.3869, 0.1920,\n",
      "        0.7466, 0.8060, 0.1160, 0.8198, 0.5568, 0.6887, 0.7552, 0.1614, 0.1667,\n",
      "        0.1870, 0.1962, 0.6870, 0.8127, 0.3192]), tensor([0.1273, 0.2640, 0.2750, 0.8481, 0.2202, 0.1492, 0.7416, 0.8086, 0.3407,\n",
      "        0.7534, 0.7807, 0.1802, 0.8452, 0.7479, 0.2477, 0.2391, 0.6666, 0.2978,\n",
      "        0.8139, 0.6983, 0.1412, 0.8059, 0.5843, 0.2559, 0.4975, 0.4368, 0.5771,\n",
      "        0.1722, 0.2217, 0.3440, 0.3027, 0.6233, 0.8586, 0.7222, 0.7680, 0.1796,\n",
      "        0.8236, 0.1707, 0.2967, 0.7787, 0.1527, 0.3986, 0.5764, 0.2989, 0.1375,\n",
      "        0.3347, 0.7787, 0.2094, 0.5389, 0.2391]), tensor([0.2375, 0.2172, 0.7405, 0.3520, 0.6223, 0.6524, 0.8440, 0.2320, 0.6797,\n",
      "        0.2987, 0.8984, 0.7816, 0.7913, 0.8408, 0.1531, 0.1152, 0.2696, 0.1730,\n",
      "        0.7754, 0.1176, 0.6563, 0.7140, 0.6093, 0.3430, 0.1993, 0.8391, 0.2172,\n",
      "        0.8150, 0.8586, 0.5236, 0.8501, 0.7821, 0.1336, 0.4457, 0.7362, 0.8483,\n",
      "        0.8023, 0.8236, 0.7456, 0.2060, 0.1663, 0.7754, 0.9146, 0.6845, 0.1571,\n",
      "        0.8170, 0.8093, 0.2930, 0.7073, 0.8257]), tensor([0.0934, 0.8526, 0.4404, 0.7210, 0.6586, 0.2706, 0.2499, 0.8112, 0.8748,\n",
      "        0.4540, 0.5886, 0.4990, 0.2740, 0.3853, 0.4200, 0.8021, 0.3639, 0.1848,\n",
      "        0.1728, 0.1639, 0.7621, 0.1958, 0.5641, 0.8274, 0.8413, 0.2035, 0.1245,\n",
      "        0.2917, 0.8624, 0.8707, 0.2702, 0.7696, 0.5238, 0.6992, 0.2144, 0.4450,\n",
      "        0.4066, 0.1596, 0.1624, 0.1542, 0.8363, 0.5837, 0.1568, 0.6425, 0.7318,\n",
      "        0.1717, 0.5881, 0.3787, 0.7242, 0.1104]), tensor([0.1847, 0.2097, 0.7815, 0.6956, 0.6235, 0.5091, 0.8781, 0.7818, 0.3119,\n",
      "        0.8559, 0.5537, 0.7478, 0.2875, 0.8104, 0.7765, 0.2684, 0.8703, 0.2104,\n",
      "        0.5515, 0.3048, 0.2379, 0.7429, 0.6648, 0.1639, 0.7543, 0.8153, 0.8453,\n",
      "        0.3230, 0.8779, 0.2137, 0.8301, 0.7859, 0.7686, 0.2621, 0.8713, 0.2227,\n",
      "        0.8294, 0.2173, 0.7785, 0.7689, 0.5628, 0.8619, 0.4048, 0.4387, 0.2550,\n",
      "        0.6219, 0.3662, 0.4244, 0.8400, 0.2103]), tensor([0.2602, 0.7712, 0.8043, 0.2011, 0.2808, 0.3235, 0.8770, 0.2653, 0.6480,\n",
      "        0.2771, 0.2612, 0.5014, 0.6583, 0.7713, 0.8211, 0.2515, 0.8646, 0.2200,\n",
      "        0.6347, 0.7360, 0.3697, 0.7190, 0.5739, 0.1711, 0.1929, 0.4941, 0.2798,\n",
      "        0.2725, 0.7213, 0.3713, 0.4410, 0.2478, 0.2003, 0.1717, 0.2218, 0.7604,\n",
      "        0.7566, 0.8128, 0.8475, 0.7850, 0.6913, 0.3144, 0.8295, 0.7368, 0.4767,\n",
      "        0.2700, 0.7995, 0.2478, 0.7583, 0.2062]), tensor([0.7264, 0.5514, 0.7418, 0.5198, 0.3363, 0.6949, 0.8390, 0.1502, 0.2865,\n",
      "        0.2513, 0.3584, 0.3455, 0.4012, 0.8055, 0.7414, 0.3702, 0.7559, 0.3915,\n",
      "        0.1211, 0.2579, 0.7701, 0.1447, 0.8424, 0.6304, 0.8036, 0.6669, 0.1575,\n",
      "        0.5481, 0.7550, 0.2070, 0.4005, 0.1572, 0.2604, 0.3354, 0.6876, 0.2570,\n",
      "        0.0860, 0.7451, 0.8239, 0.8124, 0.2128, 0.5674, 0.1356, 0.8144, 0.7847,\n",
      "        0.1597, 0.8481, 0.4622, 0.8445, 0.2022]), tensor([0.6191, 0.3250, 0.7914, 0.8366, 0.1556, 0.7337, 0.8124, 0.8932, 0.7728,\n",
      "        0.2310, 0.1801, 0.1732, 0.3624, 0.4210, 0.8020, 0.7173, 0.2216, 0.1782,\n",
      "        0.1788, 0.1265, 0.7150, 0.2319, 0.1937, 0.7609, 0.5054, 0.8034, 0.8224,\n",
      "        0.6906, 0.1185, 0.7914, 0.2695, 0.7325, 0.8749, 0.7269, 0.1618, 0.5304,\n",
      "        0.6920, 0.6496, 0.3108, 0.8576, 0.8053, 0.7880, 0.2263, 0.2756, 0.2982,\n",
      "        0.2595, 0.4121, 0.2931, 0.1013, 0.5139]), tensor([0.2956, 0.6646, 0.8877, 0.7871, 0.8403, 0.4532, 0.7700, 0.8419, 0.2855,\n",
      "        0.7486, 0.1653, 0.1022, 0.2691, 0.7794, 0.2635, 0.2566, 0.2294, 0.3620,\n",
      "        0.1668, 0.7248, 0.2482, 0.1465, 0.1334, 0.8182, 0.1675, 0.4778, 0.4376,\n",
      "        0.1325, 0.8320, 0.3323, 0.7766, 0.7953, 0.7586, 0.3034, 0.3009, 0.8453,\n",
      "        0.3761, 0.7778, 0.7118, 0.7144, 0.3115, 0.5286, 0.8552, 0.1876, 0.8463,\n",
      "        0.4873, 0.7931, 0.5743, 0.1362, 0.2855]), tensor([0.6555, 0.6167, 0.3774, 0.2069, 0.2797, 0.3630, 0.2452, 0.8041, 0.2266,\n",
      "        0.6231, 0.3083, 0.8088, 0.2509, 0.2968, 0.7992, 0.7778, 0.1454, 0.3364,\n",
      "        0.7110, 0.5829, 0.2103, 0.7374, 0.1904, 0.8237, 0.8368, 0.6997, 0.3035,\n",
      "        0.2801, 0.1720, 0.2145, 0.7178, 0.7099, 0.4258, 0.2133, 0.3310, 0.1960,\n",
      "        0.1685, 0.4644, 0.7786, 0.5901, 0.7051, 0.6701, 0.4946, 0.2336, 0.1705,\n",
      "        0.7897, 0.7842, 0.5921, 0.3596, 0.3667]), tensor([0.8152, 0.6591, 0.5879, 0.8249, 0.1731, 0.2164, 0.3840, 0.1884, 0.1643,\n",
      "        0.1020, 0.2480, 0.7081, 0.2876, 0.6286, 0.8177, 0.7714, 0.5111, 0.3828,\n",
      "        0.6407, 0.6708, 0.6120, 0.1292, 0.3131, 0.3607, 0.7239, 0.1727, 0.3649,\n",
      "        0.7510, 0.4669, 0.2164, 0.7467, 0.7319, 0.2436, 0.1738, 0.1419, 0.0899,\n",
      "        0.1532, 0.1285, 0.8523, 0.0970, 0.6581, 0.1624, 0.7219, 0.2580, 0.6630,\n",
      "        0.4332, 0.7159, 0.8068, 0.1502, 0.8782]), tensor([0.4444, 0.6331, 0.3135, 0.1750, 0.1472, 0.2480, 0.1431, 0.7574, 0.2747,\n",
      "        0.4719, 0.2181, 0.7366, 0.8157, 0.2610, 0.1417, 0.3022, 0.7879, 0.5975,\n",
      "        0.2166, 0.6432, 0.4722, 0.2103, 0.3144, 0.2476, 0.8738, 0.2355, 0.7262,\n",
      "        0.5326, 0.4550, 0.8585, 0.2322, 0.6801, 0.2714, 0.3750, 0.4169, 0.6932,\n",
      "        0.8459, 0.8351, 0.1712, 0.5621, 0.2091, 0.5660, 0.4092, 0.7060, 0.1696,\n",
      "        0.2013, 0.3129, 0.7950, 0.1679, 0.8886]), tensor([0.6622, 0.8826, 0.3364, 0.4596, 0.2354, 0.8254, 0.6718, 0.1765, 0.8515,\n",
      "        0.8692, 0.2961, 0.6633, 0.8624, 0.2795, 0.7579, 0.8784, 0.1907, 0.5891,\n",
      "        0.2463, 0.1205, 0.7174, 0.1664, 0.8134, 0.1314, 0.2210, 0.1765, 0.3711,\n",
      "        0.2976, 0.6186, 0.1674, 0.6578, 0.1495, 0.2041, 0.6970, 0.6469, 0.2314,\n",
      "        0.6045, 0.8619, 0.7534, 0.2665, 0.2255, 0.5711, 0.3750, 0.4613, 0.2848,\n",
      "        0.8535, 0.2306, 0.1660, 0.6471, 0.8103]), tensor([0.1890, 0.5874, 0.7568, 0.3440, 0.6772, 0.1955, 0.6417, 0.5900, 0.7549,\n",
      "        0.6448, 0.3373, 0.8465, 0.2682, 0.8246, 0.8513, 0.7983, 0.8220, 0.4458,\n",
      "        0.1853, 0.7710, 0.6548, 0.7982, 0.8440, 0.1669, 0.7025, 0.7478, 0.1729,\n",
      "        0.9130, 0.2190, 0.1574, 0.1849, 0.2363, 0.7190, 0.7753, 0.2555, 0.8085,\n",
      "        0.2274, 0.1848, 0.1977, 0.1471, 0.2482, 0.7825, 0.3165, 0.1849, 0.1103,\n",
      "        0.6793, 0.8344, 0.1956, 0.2003, 0.2914]), tensor([0.8566, 0.3481, 0.8612, 0.7875, 0.7961, 0.2469, 0.2420, 0.1725, 0.1558,\n",
      "        0.2276, 0.8286, 0.2541, 0.4544, 0.8757, 0.4460, 0.6146, 0.8425, 0.2714,\n",
      "        0.6870, 0.7945, 0.1956, 0.7121, 0.7987, 0.3159, 0.8855, 0.1849, 0.1906,\n",
      "        0.7316, 0.1548, 0.6003, 0.5951, 0.8004, 0.3249, 0.7228, 0.3159, 0.1468,\n",
      "        0.8297, 0.4430, 0.3813, 0.2453, 0.1977, 0.8060, 0.7400, 0.2224, 0.8592,\n",
      "        0.5810, 0.5558, 0.4225, 0.2178, 0.6127]), tensor([0.1245, 0.1557, 0.1843, 0.9125, 0.2816, 0.8698, 0.8653, 0.6603, 0.2737,\n",
      "        0.5739, 0.1099, 0.6822, 0.8022, 0.4502, 0.1113, 0.8523, 0.1796, 0.7832,\n",
      "        0.5485, 0.8994, 0.7734, 0.1779, 0.2671, 0.8363, 0.4690, 0.6838, 0.2646,\n",
      "        0.8153, 0.7866, 0.8364, 0.2799, 0.6740, 0.1851, 0.6245, 0.2194, 0.8521,\n",
      "        0.7636, 0.1452, 0.3745, 0.7796, 0.1499, 0.8216, 0.1816, 0.8848, 0.7964,\n",
      "        0.7497, 0.2890, 0.8648, 0.7932, 0.7680]), tensor([0.7506, 0.1702, 0.8168, 0.2904, 0.8550, 0.3537, 0.8792, 0.1617, 0.7911,\n",
      "        0.5163, 0.2412, 0.2029, 0.6573, 0.7144, 0.1897, 0.8242, 0.7992, 0.1800,\n",
      "        0.8090, 0.4585, 0.4898, 0.4380, 0.2204, 0.7663, 0.6034, 0.7925, 0.8237,\n",
      "        0.3623, 0.1801, 0.6204, 0.2517, 0.2413, 0.2448, 0.2234, 0.7136, 0.8297,\n",
      "        0.8120, 0.7692, 0.2030, 0.1620, 0.2522, 0.8643, 0.1615, 0.6663, 0.7943,\n",
      "        0.4782, 0.6185, 0.3108, 0.1910, 0.4443]), tensor([0.7561, 0.2952, 0.5584, 0.2700, 0.8335, 0.2391, 0.1716, 0.8153, 0.1242,\n",
      "        0.6656, 0.6783, 0.7919, 0.8462, 0.2617, 0.8199, 0.6465, 0.8425, 0.1874,\n",
      "        0.5697, 0.2920, 0.4591, 0.6819, 0.6367, 0.8235, 0.7547, 0.1469, 0.4533,\n",
      "        0.2213, 0.2243, 0.7605, 0.8345, 0.2032, 0.1712, 0.1513, 0.1779, 0.7694,\n",
      "        0.6979, 0.8227, 0.8223, 0.2311, 0.2300, 0.8413, 0.8368, 0.8247, 0.8066,\n",
      "        0.4475, 0.2524, 0.6957, 0.2451, 0.4920]), tensor([0.1799, 0.7897, 0.5043, 0.3916, 0.4161, 0.1653, 0.6590, 0.4689, 0.7983,\n",
      "        0.1462, 0.7953, 0.7206, 0.4590, 0.1746, 0.6817, 0.1592, 0.4995, 0.6394,\n",
      "        0.1237, 0.2026, 0.5075, 0.8203, 0.7242, 0.9071, 0.2125, 0.1389, 0.2966,\n",
      "        0.7436, 0.6990, 0.8712, 0.7989, 0.4576, 0.1040, 0.5379, 0.7142, 0.8340,\n",
      "        0.2858, 0.2621, 0.4936, 0.7690, 0.3249, 0.7000, 0.4900, 0.8225, 0.9252,\n",
      "        0.2071, 0.5826, 0.6368, 0.8547, 0.1481]), tensor([0.6615, 0.8664, 0.4644, 0.7727, 0.6812, 0.2257, 0.8385, 0.1427, 0.1193,\n",
      "        0.3161, 0.8646, 0.1748, 0.7569, 0.2690, 0.2254, 0.9052, 0.1865, 0.8957,\n",
      "        0.2761, 0.1863, 0.1451, 0.3619, 0.2237, 0.8059, 0.2576, 0.5471, 0.6287,\n",
      "        0.7565, 0.8607, 0.5677, 0.4600, 0.2407, 0.7005, 0.4121, 0.8676, 0.2529,\n",
      "        0.5051, 0.4312, 0.7896, 0.1859, 0.1754, 0.8242, 0.3763, 0.6296, 0.7937,\n",
      "        0.2357, 0.5798, 0.8453, 0.7455, 0.2336]), tensor([0.1608, 0.5405, 0.8247, 0.7646, 0.3569, 0.1470, 0.1866, 0.3357, 0.4723,\n",
      "        0.8196, 0.7702, 0.1463, 0.1546, 0.6611, 0.8354, 0.2608, 0.8232, 0.2397,\n",
      "        0.5638, 0.4016, 0.6774, 0.4160, 0.8059, 0.7906, 0.3300, 0.1838, 0.1458,\n",
      "        0.8292, 0.8983, 0.1073, 0.7963, 0.3420, 0.5907, 0.7236, 0.6430, 0.5345,\n",
      "        0.6142, 0.1772, 0.5619, 0.2538, 0.1795, 0.7416, 0.3977, 0.2435, 0.8101,\n",
      "        0.7193, 0.3131, 0.7946, 0.1556, 0.5490]), tensor([0.6035, 0.7632, 0.1238, 0.5298, 0.2605, 0.1306, 0.1993, 0.7316, 0.2970,\n",
      "        0.2499, 0.8261, 0.6325, 0.8009, 0.2918, 0.5174, 0.3405, 0.2331, 0.6708,\n",
      "        0.7323, 0.8557, 0.1905, 0.2132, 0.8338, 0.7412, 0.4314, 0.7334, 0.6597,\n",
      "        0.7983, 0.8463, 0.8055, 0.2049, 0.6127, 0.6672, 0.4946, 0.8547, 0.7334,\n",
      "        0.4905, 0.8493, 0.6357, 0.7799, 0.3488, 0.3939, 0.8510, 0.4614, 0.4102,\n",
      "        0.2150, 0.1525, 0.3191, 0.5231, 0.1847]), tensor([0.8510, 0.6840, 0.2619, 0.8319, 0.2267, 0.2929, 0.1864, 0.4638, 0.8020,\n",
      "        0.4091, 0.8462, 0.5884, 0.6966, 0.2358, 0.1708, 0.2947, 0.6969, 0.1641,\n",
      "        0.8105, 0.1586, 0.8653, 0.7805, 0.3693, 0.3689, 0.5467, 0.1624, 0.3496,\n",
      "        0.3488, 0.2703, 0.7067, 0.2255, 0.8687, 0.1711, 0.7680, 0.6829, 0.2268,\n",
      "        0.3953, 0.1958, 0.7434, 0.2029, 0.2274, 0.8547, 0.1345, 0.7153, 0.8626,\n",
      "        0.9018, 0.7380, 0.6349, 0.8184, 0.7301]), tensor([0.7318, 0.2680, 0.2166, 0.7875, 0.7353, 0.4538, 0.8469, 0.6869, 0.1467,\n",
      "        0.2579, 0.5449, 0.8368, 0.3242, 0.4810, 0.7624, 0.1435, 0.4798, 0.1579,\n",
      "        0.8185, 0.8476, 0.2956, 0.1385, 0.2436, 0.5185, 0.3696, 0.8048, 0.7774,\n",
      "        0.7478, 0.7753, 0.2926, 0.6496, 0.3333, 0.3862, 0.3382, 0.6662, 0.1817,\n",
      "        0.2153, 0.6666, 0.4052, 0.8207, 0.3848, 0.5644, 0.4884, 0.6586, 0.7128,\n",
      "        0.8765, 0.7753, 0.7214, 0.2357, 0.8366]), tensor([0.1555, 0.1261, 0.6329, 0.4500, 0.1898, 0.8462, 0.3395, 0.4185, 0.4485,\n",
      "        0.8634, 0.8360, 0.8211, 0.1841, 0.1531, 0.8422, 0.8310, 0.7786, 0.6072,\n",
      "        0.2409, 0.2166, 0.8389, 0.7247, 0.2152, 0.2202, 0.8560, 0.6548, 0.2218,\n",
      "        0.7838, 0.6603, 0.1411, 0.8286, 0.3966, 0.7273, 0.7305, 0.8131, 0.7692,\n",
      "        0.1427, 0.7918, 0.3625, 0.7435, 0.5355, 0.3095, 0.5208, 0.2126, 0.8622,\n",
      "        0.2572, 0.2852, 0.2594, 0.2204, 0.6769]), tensor([0.8558, 0.8269, 0.1852, 0.7898, 0.2236, 0.8210, 0.7266, 0.7617, 0.2716,\n",
      "        0.3580, 0.8667, 0.7056, 0.6737, 0.2603, 0.8003, 0.7318, 0.1873, 0.1438,\n",
      "        0.5512, 0.4601, 0.1841, 0.5745, 0.1540, 0.8820, 0.3516, 0.3772, 0.5424,\n",
      "        0.3180, 0.4053, 0.4836, 0.1951, 0.5738, 0.1180, 0.8319, 0.3435, 0.3386,\n",
      "        0.7561, 0.1273, 0.7985, 0.9029, 0.3380, 0.6001, 0.2112, 0.5684, 0.3661,\n",
      "        0.8481, 0.2716, 0.8580, 0.4405, 0.6553]), tensor([0.8512, 0.5727, 0.3885, 0.1619, 0.4382, 0.5197, 0.2774, 0.7039, 0.8829,\n",
      "        0.3642, 0.4230, 0.3962, 0.5721, 0.7958, 0.7708, 0.8147, 0.7387, 0.4335,\n",
      "        0.8949, 0.1719, 0.1839, 0.1570, 0.2660, 0.2597, 0.7210, 0.3491, 0.1890,\n",
      "        0.2672, 0.1448, 0.1991, 0.2679, 0.9081, 0.6046, 0.3206, 0.2115, 0.1467,\n",
      "        0.8938, 0.1876, 0.8325, 0.5404, 0.1374, 0.1383, 0.1503, 0.7994, 0.2447,\n",
      "        0.2517, 0.5041, 0.1781, 0.3856, 0.4211]), tensor([0.4697, 0.2183, 0.8731, 0.6759, 0.7999, 0.5320, 0.8390, 0.1990, 0.2012,\n",
      "        0.2341, 0.4654, 0.8452, 0.1696, 0.2094, 0.8026, 0.6827, 0.8897, 0.5164,\n",
      "        0.1707, 0.5198, 0.4335, 0.8075, 0.8418, 0.7765, 0.7273, 0.2774, 0.1667,\n",
      "        0.1737, 0.3891, 0.1777, 0.2530, 0.2231, 0.1826, 0.4258, 0.2669, 0.5131,\n",
      "        0.1660, 0.8258, 0.7769, 0.4918, 0.3113, 0.8531, 0.8268, 0.5961, 0.4719,\n",
      "        0.1597, 0.3445, 0.1457, 0.1778, 0.2217]), tensor([0.1801, 0.6116, 0.3466, 0.5041, 0.1631, 0.8029, 0.8349, 0.5920, 0.9027,\n",
      "        0.3201, 0.1957, 0.2978, 0.8697, 0.1248, 0.8519, 0.1783, 0.8045, 0.5737,\n",
      "        0.8327, 0.8523, 0.8549, 0.4325, 0.3291, 0.3336, 0.7453, 0.7348, 0.1917,\n",
      "        0.8454, 0.1396, 0.6387, 0.7307, 0.3087, 0.8900, 0.3269, 0.8365, 0.6382,\n",
      "        0.2075, 0.7613, 0.7704, 0.2612, 0.8696, 0.8966, 0.5060, 0.1911, 0.8514,\n",
      "        0.1552, 0.2584, 0.2382, 0.2453, 0.9068]), tensor([0.1811, 0.7903, 0.7545, 0.1880, 0.1385, 0.2908, 0.2311, 0.5270, 0.1657,\n",
      "        0.3453, 0.7818, 0.1669, 0.1899, 0.2153, 0.5406, 0.1558, 0.6747, 0.7677,\n",
      "        0.5829, 0.3374, 0.7403, 0.2213, 0.5497, 0.3292, 0.5020, 0.4035, 0.3554,\n",
      "        0.8276, 0.7023, 0.3534, 0.8374, 0.1306, 0.7298, 0.1840, 0.1437, 0.4351,\n",
      "        0.8501, 0.2088, 0.1157, 0.2533, 0.6608, 0.3825, 0.8219, 0.8351, 0.6137,\n",
      "        0.8388, 0.1910, 0.1820, 0.6074, 0.8795]), tensor([0.8061, 0.2385, 0.3568, 0.3893, 0.8229, 0.7722, 0.2752, 0.1298, 0.5930,\n",
      "        0.2022, 0.8054, 0.1791, 0.6188, 0.6108, 0.3588, 0.8556, 0.2767, 0.8634,\n",
      "        0.5950, 0.8407, 0.8189, 0.8013, 0.8398, 0.7457, 0.4944, 0.5446, 0.2666,\n",
      "        0.7771, 0.3587, 0.9043, 0.3970, 0.3954, 0.8716, 0.5428, 0.8221, 0.1320,\n",
      "        0.8807, 0.7781, 0.2591, 0.6908, 0.2317, 0.6151, 0.2022, 0.5748, 0.7015,\n",
      "        0.6784, 0.8398, 0.1999, 0.8505, 0.8177]), tensor([0.1315, 0.5261, 0.4629, 0.1828, 0.7920, 0.6240, 0.3661, 0.6301, 0.2811,\n",
      "        0.2354, 0.4222, 0.7291, 0.7977, 0.8678, 0.8095, 0.1150, 0.5464, 0.3780,\n",
      "        0.2427, 0.1270, 0.8714, 0.3066, 0.4066, 0.1434, 0.3212, 0.5645, 0.7391,\n",
      "        0.5680, 0.5165, 0.7229, 0.8408, 0.4011, 0.7340, 0.2642, 0.1658, 0.8200,\n",
      "        0.1576, 0.3221, 0.8615, 0.4447, 0.6905, 0.8792, 0.7534, 0.7085, 0.8636,\n",
      "        0.4508, 0.7179, 0.4644, 0.8512, 0.3809]), tensor([0.8289, 0.6565, 0.7935, 0.3096, 0.8631, 0.8626, 0.6928, 0.7729, 0.3531,\n",
      "        0.2376, 0.8771, 0.8251, 0.3281, 0.1754, 0.1767, 0.8329, 0.8745, 0.6918,\n",
      "        0.3728, 0.7090, 0.8439, 0.7297, 0.7286, 0.1774, 0.1194, 0.1784, 0.5104,\n",
      "        0.7819, 0.1964, 0.2333, 0.7816, 0.7481, 0.3178, 0.1610, 0.2654, 0.8417,\n",
      "        0.8293, 0.3051, 0.2739, 0.4205, 0.8731, 0.8326, 0.8541, 0.7308, 0.4132,\n",
      "        0.8860, 0.3913, 0.8500, 0.3359, 0.5648])]\n"
     ]
    }
   ],
   "source": [
    "prediction = predictions(reviews)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I always wrote this series off as being a comp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie was so poorly written and directed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The most interesting thing about Miryang (Secr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when i first read about \"berlin am meer\" i did...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>This is the kind of picture John Lassiter woul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>A MUST SEE! I saw WHIPPED at a press screening...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>NBC should be ashamed. I would not allow my ch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>This movie is a clumsy mishmash of various gho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Formula movie about the illegitimate son of a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     I always wrote this series off as being a comp...      0\n",
       "1     1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...      0\n",
       "2     This movie was so poorly written and directed ...      0\n",
       "3     The most interesting thing about Miryang (Secr...      1\n",
       "4     when i first read about \"berlin am meer\" i did...      0\n",
       "...                                                 ...    ...\n",
       "4995  This is the kind of picture John Lassiter woul...      1\n",
       "4996  A MUST SEE! I saw WHIPPED at a press screening...      1\n",
       "4997  NBC should be ashamed. I would not allow my ch...      0\n",
       "4998  This movie is a clumsy mishmash of various gho...      0\n",
       "4999  Formula movie about the illegitimate son of a ...      0\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5ac5e08ce62d121e07a1d4fa1bf09afe1a72871fda0686362050c3d0296ad75"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
