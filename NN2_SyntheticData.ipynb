{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1194</td>\n",
       "      <td>attempt line i let the nuclear scene beautiful...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1128</td>\n",
       "      <td>self from with could socially least in the old...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1281</td>\n",
       "      <td>gambling for about jungle one the plays have t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>879</td>\n",
       "      <td>cancelled be slashers trying wasn all go more ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>729</td>\n",
       "      <td>semester the think but lacks creek of it of go...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>974</td>\n",
       "      <td>big there work just read was of angrier saw am...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>241</td>\n",
       "      <td>movies this danny understand to propaganda on ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>27</td>\n",
       "      <td>old stop with shots it in in actors sargent ke...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>647</td>\n",
       "      <td>not crazy that film didn for did which victori...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>712</td>\n",
       "      <td>film for becomes wusses and has bad both favor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                             review  sentiment\n",
       "0      1194  attempt line i let the nuclear scene beautiful...          0\n",
       "1      1128  self from with could socially least in the old...          0\n",
       "2      1281  gambling for about jungle one the plays have t...          0\n",
       "3       879  cancelled be slashers trying wasn all go more ...          0\n",
       "4       729  semester the think but lacks creek of it of go...          0\n",
       "...     ...                                                ...        ...\n",
       "4995    974  big there work just read was of angrier saw am...          0\n",
       "4996    241  movies this danny understand to propaganda on ...          1\n",
       "4997     27  old stop with shots it in in actors sargent ke...          1\n",
       "4998    647  not crazy that film didn for did which victori...          1\n",
       "4999    712  film for becomes wusses and has bad both favor...          0\n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"~/Desktop/ece684/synthetic_reviews.csv\", sep=\"\\t\")\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Reformats Reviews\n",
    "\n",
    "\n",
    "def cleanReview(rev):\n",
    "    # Removing anything within a HTML tag\n",
    "    edited_rev = re.compile(r\"<[^>]+>\").sub(\" \", rev)\n",
    "    # Removing Punctuation\n",
    "    edited_rev = re.sub(r\"[^\\w\\s]\", \" \", edited_rev)\n",
    "    # Removing Numbers\n",
    "    edited_rev = re.sub(r\"[0-9]\", \" \", edited_rev)\n",
    "    # Removing single characters\n",
    "    edited_rev = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", edited_rev)\n",
    "    # Removing multiple spaces\n",
    "    edited_rev = re.sub(r\"\\s+\", \" \", edited_rev)\n",
    "\n",
    "    return edited_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"review\"] = reviews[\"review\"].apply(lambda x: x.replace(\"wouldn't\", \"would not\"))\n",
    "reviews[\"review\"] = reviews[\"review\"].apply(lambda x: x.replace(\"won't\", \"will not\"))\n",
    "reviews[\"review\"] = reviews[\"review\"].apply(lambda x: x.replace(\"can't\", \"can not\"))\n",
    "reviews[\"review\"] = reviews[\"review\"].apply(lambda x: x.replace(\"couldn't\", \"could not\"))\n",
    "reviews[\"review\"] = reviews[\"review\"].apply(lambda x: x.replace(\"I'm\", \"I am\"))\n",
    "reviews[\"review\"] = reviews[\"review\"].apply(lambda x: x.replace(\"ain't\", \"is not\"))\n",
    "reviews[\"review\"] = reviews[\"review\"].apply(lambda x: x.replace(\"shouldn't\", \"should not\"))\n",
    "reviews[\"review\"] = reviews[\"review\"].apply(lambda x: x.replace(\"(\\w+)'ll\", \"\\g<1> will\"))\n",
    "reviews[\"review\"] = reviews[\"review\"].apply(lambda x: x.replace(\"(\\w+)'ve\", \"\\g<1> have\"))\n",
    "reviews[\"review\"] = reviews[\"review\"].apply(lambda x: x.replace(\"(\\w+)'s\", \"\\g<1> is\"))\n",
    "reviews[\"review\"] = reviews[\"review\"].apply(lambda x: x.replace(\"(\\w+)'re\", \"\\g<1> are\"))\n",
    "reviews[\"review\"] = reviews[\"review\"].apply(lambda x: x.replace(\"(\\w+)'d\", \"\\g<1> would\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list = []\n",
    "\n",
    "# Adding all of the movie reviews to a list\n",
    "for r in reviews[\"review\"]:\n",
    "    review_list.append(cleanReview(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start of tokenizing\n",
    "words = \" \".join(review_list)\n",
    "words = list(words.split(\" \"))\n",
    "\n",
    "#Getting word counts\n",
    "totalCounts = {}\n",
    "for j in range(len(words)):\n",
    "    if words[j] not in totalCounts:\n",
    "        totalCounts[words[j]] = 1\n",
    "    if words[j] in totalCounts:\n",
    "        totalCounts[words[j]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting dictionary to have highest word counts first\n",
    "sorted_counts=dict(sorted(totalCounts.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the highest word count value be 1, etc.\n",
    "w_to_i = {w:i+1 for i, (w) in enumerate(sorted_counts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the words by replacing the word with its given integer\n",
    "X = []\n",
    "for m in review_list:\n",
    "    r = [w_to_i[w] for w in m.split()]\n",
    "    X.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221.1912"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "lengths = []\n",
    "for i in range(len(X)):\n",
    "    l = len(X[i])\n",
    "    lengths.append(l)\n",
    "statistics.mean(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding for different review lengths\n",
    "X_array = np.zeros((len(X), 221), dtype =int)\n",
    "for i, rev2 in enumerate(X):\n",
    "    rev_length = len(rev2)\n",
    "\n",
    "    if rev_length <= 221:\n",
    "        zeros = list(np.zeros(221-rev_length))\n",
    "        mat = zeros + rev2\n",
    "    elif rev_length > 221:\n",
    "        mat = rev2[0:221]\n",
    "    X_array[i,:] = np.array(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a training and test data sets\n",
    "y = reviews[\"sentiment\"]\n",
    "\n",
    "y = np.array(list(map(lambda x: 1 if x == 1 else 0, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_array, y, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# create Tensor Dataset\n",
    "train_data = TensorDataset(torch.LongTensor(X_train), torch.LongTensor(y_train))\n",
    "valid_data=TensorDataset(torch.LongTensor(X_valid), torch.LongTensor(y_valid))\n",
    "test_data = TensorDataset(torch.LongTensor(X_test), torch.LongTensor(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader\n",
    "batch_size=50 #50\n",
    "train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentalLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_words, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(num_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True\n",
    "        )\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.4) #0.3\n",
    "\n",
    "        # Linear and sigmoid layer\n",
    "        self.fc1 = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size()\n",
    "\n",
    "        #   Embadding and LSTM output\n",
    "        embedd = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embedd, hidden)\n",
    "\n",
    "        # stack up the lstm output\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # dropout and fully connected layers\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout(out)\n",
    "        '''out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)'''\n",
    "        sig_out = self.sigmoid(out)\n",
    "\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize Hidden STATE\"\"\"\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentalLSTM(\n",
      "  (embedding): Embedding(1105958, 221)\n",
      "  (lstm): LSTM(221, 250, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc1): Linear(in_features=250, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "#vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
    "num_words = len(words)\n",
    "output_size = 1\n",
    "embedding_dim = 221\n",
    "hidden_dim = 250 #256 250\n",
    "n_layers = 2 #2\n",
    "\n",
    "net = SentimentalLSTM(num_words, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10... Step: 100... Loss: 0.688107... Val Loss: 0.692925\n",
      "Epoch: 4/10... Step: 200... Loss: 0.684471... Val Loss: 0.692397\n",
      "Epoch: 5/10... Step: 300... Loss: 0.664346... Val Loss: 0.685722\n",
      "Epoch: 7/10... Step: 400... Loss: 0.554848... Val Loss: 0.587087\n",
      "Epoch: 9/10... Step: 500... Loss: 0.650548... Val Loss: 0.468002\n",
      "Epoch: 10/10... Step: 600... Loss: 0.440894... Val Loss: 0.459061\n"
     ]
    }
   ],
   "source": [
    "lr=0.0001 #0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 10 # is approx where I noticed the validation loss stop decreasing, 3\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs=inputs.cuda()\n",
    "            labels=labels.cuda()\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "        \n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                inputs, labels = inputs, labels \n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.435\n",
      "Test accuracy: 0.817\n"
     ]
    }
   ],
   "source": [
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "\n",
    "    output, h = net(inputs, h)\n",
    "\n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.817\n"
     ]
    }
   ],
   "source": [
    "train_acc = num_correct/len(train_loader.dataset)\n",
    "print(\"Train accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(reviews):\n",
    "    review_list2 = []\n",
    "\n",
    "    # Adding all of the movie reviews to a list\n",
    "    for r2 in reviews[\"review\"]:\n",
    "        review_list2.append(cleanReview(r2))\n",
    "\n",
    "    # Encoding the words by replacing the word with its given integer\n",
    "    X2 = []\n",
    "    for m2 in review_list2:\n",
    "        r2 = [w_to_i[w2] for w2 in m2.split()]\n",
    "        X2.append(r)\n",
    "\n",
    "    #Padding for different review lengths\n",
    "    X_array2 = np.zeros((len(X2), 221), dtype =int)\n",
    "    for i2, rev8 in enumerate(X2):\n",
    "        rev_length2 = len(rev8)\n",
    "\n",
    "        if rev_length2 <= 221:\n",
    "            zeros2 = list(np.zeros(221-rev_length2))\n",
    "            mat2 = zeros2 + rev8\n",
    "        elif rev_length2 > 221:\n",
    "            mat2 = rev8[0:221]\n",
    "        X_array2[i2,:] = np.array(mat2)\n",
    "\n",
    "    counter = 0\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    final_probs = []\n",
    "    with torch.no_grad():\n",
    "        # batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs=inputs.cuda()\n",
    "                labels=labels.cuda()\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "        \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            final_probs.append(output)\n",
    "\n",
    "        return final_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.2858, 0.8430, 0.7066, 0.1627, 0.5354, 0.7859, 0.1638, 0.3929, 0.8117,\n",
      "        0.8063, 0.2262, 0.8595, 0.7969, 0.8467, 0.8300, 0.3722, 0.8839, 0.6210,\n",
      "        0.8147, 0.2015, 0.8303, 0.2208, 0.2096, 0.8066, 0.8634, 0.1695, 0.8436,\n",
      "        0.8700, 0.1722, 0.3576, 0.6575, 0.8552, 0.1606, 0.8099, 0.3998, 0.6622,\n",
      "        0.4305, 0.1939, 0.8112, 0.7640, 0.6031, 0.1888, 0.8462, 0.6844, 0.2827,\n",
      "        0.1378, 0.7942, 0.7163, 0.8433, 0.8506]), tensor([0.7668, 0.4739, 0.8199, 0.7997, 0.6442, 0.8627, 0.8560, 0.7416, 0.4116,\n",
      "        0.8116, 0.1251, 0.8396, 0.1847, 0.8031, 0.8595, 0.2980, 0.6873, 0.1682,\n",
      "        0.2212, 0.7987, 0.7941, 0.9019, 0.7900, 0.7873, 0.1510, 0.1704, 0.2554,\n",
      "        0.2221, 0.1739, 0.3663, 0.3053, 0.8765, 0.8667, 0.6595, 0.1449, 0.8365,\n",
      "        0.8543, 0.2647, 0.1431, 0.2348, 0.4740, 0.8054, 0.7681, 0.7320, 0.8961,\n",
      "        0.7796, 0.2212, 0.8743, 0.6902, 0.8722]), tensor([0.1716, 0.5915, 0.8472, 0.8663, 0.1582, 0.1850, 0.1117, 0.2438, 0.8215,\n",
      "        0.7638, 0.8304, 0.7556, 0.8544, 0.8779, 0.2314, 0.4880, 0.2454, 0.8169,\n",
      "        0.2035, 0.1950, 0.6628, 0.1741, 0.2571, 0.8045, 0.8511, 0.8398, 0.1845,\n",
      "        0.8521, 0.6153, 0.1431, 0.7517, 0.8253, 0.1551, 0.1683, 0.8165, 0.7553,\n",
      "        0.8415, 0.8916, 0.2492, 0.1413, 0.2070, 0.8298, 0.2105, 0.7404, 0.1348,\n",
      "        0.7755, 0.7678, 0.8530, 0.5047, 0.8109]), tensor([0.8573, 0.8614, 0.8432, 0.8499, 0.5001, 0.7937, 0.2571, 0.7994, 0.5564,\n",
      "        0.1759, 0.2508, 0.3259, 0.6502, 0.4207, 0.1547, 0.8136, 0.8531, 0.4435,\n",
      "        0.1502, 0.2038, 0.8174, 0.8399, 0.7897, 0.1628, 0.8362, 0.2786, 0.4320,\n",
      "        0.1735, 0.5567, 0.2209, 0.7200, 0.7862, 0.2671, 0.2408, 0.1550, 0.8233,\n",
      "        0.8784, 0.8680, 0.8700, 0.1758, 0.2591, 0.5306, 0.2204, 0.7255, 0.5957,\n",
      "        0.8541, 0.8222, 0.8337, 0.1517, 0.1650]), tensor([0.2446, 0.6959, 0.2160, 0.3809, 0.8011, 0.8716, 0.8335, 0.8579, 0.4072,\n",
      "        0.8702, 0.1740, 0.8120, 0.8364, 0.2191, 0.8537, 0.8576, 0.8391, 0.8383,\n",
      "        0.3979, 0.1202, 0.5426, 0.7403, 0.4038, 0.5278, 0.5384, 0.8694, 0.7074,\n",
      "        0.8023, 0.2737, 0.3615, 0.2038, 0.7101, 0.2150, 0.1941, 0.2137, 0.8409,\n",
      "        0.1695, 0.8319, 0.7357, 0.2385, 0.1137, 0.8594, 0.8420, 0.8269, 0.1143,\n",
      "        0.6955, 0.8562, 0.6676, 0.1258, 0.1588]), tensor([0.8300, 0.6002, 0.1611, 0.1789, 0.8543, 0.2218, 0.7768, 0.8446, 0.7055,\n",
      "        0.1986, 0.1663, 0.4881, 0.1268, 0.1853, 0.5536, 0.6813, 0.2736, 0.1472,\n",
      "        0.2309, 0.1263, 0.1862, 0.2993, 0.8442, 0.4327, 0.3859, 0.5782, 0.1816,\n",
      "        0.8338, 0.2585, 0.2423, 0.1763, 0.1875, 0.4357, 0.8628, 0.8062, 0.7596,\n",
      "        0.1180, 0.1510, 0.4583, 0.7837, 0.1503, 0.7451, 0.6740, 0.7896, 0.8719,\n",
      "        0.2885, 0.8197, 0.1719, 0.2291, 0.8814]), tensor([0.7929, 0.8240, 0.2807, 0.7889, 0.6564, 0.4068, 0.8636, 0.2013, 0.8594,\n",
      "        0.3735, 0.6131, 0.8047, 0.4310, 0.4348, 0.8622, 0.8380, 0.4658, 0.2444,\n",
      "        0.4031, 0.7428, 0.7480, 0.4929, 0.6935, 0.7024, 0.1805, 0.7322, 0.7797,\n",
      "        0.1962, 0.2738, 0.7187, 0.2089, 0.3731, 0.1855, 0.2246, 0.7858, 0.6617,\n",
      "        0.7178, 0.1350, 0.4435, 0.9004, 0.8255, 0.5700, 0.7617, 0.6364, 0.1779,\n",
      "        0.7863, 0.2111, 0.2146, 0.6255, 0.8622]), tensor([0.6931, 0.2495, 0.8602, 0.2571, 0.1528, 0.7883, 0.7965, 0.8638, 0.1328,\n",
      "        0.8300, 0.7880, 0.7648, 0.3167, 0.1801, 0.1992, 0.2918, 0.6708, 0.7910,\n",
      "        0.2177, 0.1427, 0.1467, 0.8335, 0.1825, 0.5382, 0.1675, 0.7671, 0.7074,\n",
      "        0.8646, 0.7559, 0.8383, 0.2169, 0.1922, 0.8147, 0.7796, 0.8803, 0.2260,\n",
      "        0.7833, 0.2700, 0.1571, 0.1651, 0.4073, 0.8509, 0.8459, 0.1646, 0.2558,\n",
      "        0.7493, 0.3797, 0.8354, 0.7699, 0.8268]), tensor([0.6198, 0.1967, 0.8317, 0.8631, 0.8526, 0.4078, 0.1435, 0.8626, 0.7287,\n",
      "        0.5609, 0.1945, 0.7629, 0.5721, 0.8669, 0.1758, 0.7033, 0.2260, 0.1768,\n",
      "        0.3889, 0.1571, 0.7917, 0.8659, 0.2609, 0.8284, 0.1651, 0.8273, 0.2061,\n",
      "        0.1379, 0.7153, 0.2953, 0.1647, 0.1582, 0.8769, 0.8466, 0.8191, 0.1330,\n",
      "        0.2647, 0.2990, 0.8107, 0.7211, 0.1461, 0.6818, 0.8860, 0.8601, 0.8157,\n",
      "        0.1114, 0.3344, 0.5859, 0.8655, 0.1557]), tensor([0.1251, 0.8374, 0.1932, 0.7956, 0.4926, 0.8456, 0.3048, 0.2793, 0.8783,\n",
      "        0.8271, 0.7602, 0.5000, 0.8424, 0.3896, 0.4958, 0.1726, 0.8617, 0.7772,\n",
      "        0.2838, 0.1376, 0.8275, 0.8507, 0.2115, 0.8025, 0.8005, 0.7522, 0.2119,\n",
      "        0.1300, 0.1264, 0.2251, 0.1427, 0.8208, 0.8140, 0.7533, 0.2865, 0.2707,\n",
      "        0.2493, 0.8008, 0.8225, 0.8299, 0.8515, 0.1293, 0.7748, 0.8607, 0.4991,\n",
      "        0.2155, 0.6572, 0.3112, 0.6847, 0.8501]), tensor([0.1551, 0.7590, 0.8622, 0.7784, 0.8301, 0.5337, 0.3391, 0.1569, 0.8291,\n",
      "        0.8632, 0.1764, 0.8594, 0.1661, 0.1600, 0.1343, 0.2656, 0.5766, 0.6519,\n",
      "        0.8443, 0.2774, 0.8403, 0.3659, 0.7528, 0.8788, 0.7073, 0.2333, 0.8563,\n",
      "        0.2091, 0.1723, 0.1742, 0.1887, 0.3422, 0.5788, 0.8512, 0.6071, 0.7148,\n",
      "        0.8505, 0.8576, 0.1912, 0.7507, 0.8455, 0.3562, 0.1283, 0.3999, 0.1256,\n",
      "        0.7832, 0.5320, 0.2658, 0.5726, 0.6774]), tensor([0.4486, 0.3318, 0.6512, 0.8355, 0.8575, 0.8432, 0.3524, 0.2105, 0.2986,\n",
      "        0.1917, 0.7877, 0.1755, 0.1652, 0.1773, 0.2861, 0.1435, 0.7619, 0.2113,\n",
      "        0.2315, 0.8365, 0.6889, 0.4179, 0.8013, 0.1761, 0.2040, 0.1606, 0.6566,\n",
      "        0.8371, 0.7489, 0.1349, 0.8277, 0.8350, 0.8535, 0.5856, 0.6312, 0.1883,\n",
      "        0.6888, 0.4690, 0.1487, 0.5802, 0.8039, 0.8633, 0.1234, 0.1358, 0.2217,\n",
      "        0.0953, 0.1702, 0.1821, 0.1402, 0.2295]), tensor([0.2747, 0.1394, 0.8288, 0.8647, 0.8447, 0.2156, 0.1811, 0.2316, 0.1756,\n",
      "        0.8483, 0.8214, 0.8328, 0.2062, 0.1592, 0.7984, 0.8299, 0.8477, 0.8350,\n",
      "        0.1262, 0.1592, 0.2924, 0.2038, 0.8943, 0.1427, 0.2784, 0.1693, 0.8276,\n",
      "        0.7534, 0.7179, 0.1795, 0.6016, 0.8346, 0.3648, 0.6095, 0.7870, 0.8182,\n",
      "        0.1789, 0.8555, 0.7515, 0.2421, 0.1700, 0.2235, 0.8527, 0.1681, 0.5126,\n",
      "        0.1281, 0.7742, 0.2710, 0.6959, 0.7667]), tensor([0.8711, 0.2016, 0.1584, 0.8005, 0.8620, 0.2302, 0.8115, 0.8215, 0.7765,\n",
      "        0.7876, 0.8159, 0.2115, 0.8135, 0.8552, 0.8713, 0.4444, 0.6056, 0.8589,\n",
      "        0.3456, 0.2901, 0.2975, 0.8418, 0.5379, 0.7483, 0.1078, 0.8498, 0.7897,\n",
      "        0.8128, 0.6622, 0.8292, 0.8385, 0.1502, 0.1387, 0.7071, 0.1551, 0.8848,\n",
      "        0.8169, 0.1718, 0.2722, 0.8321, 0.1314, 0.3676, 0.6691, 0.8217, 0.6835,\n",
      "        0.7858, 0.8776, 0.7154, 0.7994, 0.1952]), tensor([0.5623, 0.1352, 0.6208, 0.1446, 0.8009, 0.8232, 0.8743, 0.7059, 0.8647,\n",
      "        0.8079, 0.8445, 0.8933, 0.6621, 0.8746, 0.8554, 0.7407, 0.8995, 0.7061,\n",
      "        0.8455, 0.3817, 0.8579, 0.8376, 0.8815, 0.7575, 0.1454, 0.3919, 0.8620,\n",
      "        0.3892, 0.8039, 0.8244, 0.5472, 0.8715, 0.6951, 0.8693, 0.3849, 0.2254,\n",
      "        0.8119, 0.6038, 0.1866, 0.8569, 0.1879, 0.5814, 0.2469, 0.8301, 0.4152,\n",
      "        0.8290, 0.7849, 0.8595, 0.8109, 0.1739]), tensor([0.8131, 0.8075, 0.7248, 0.2019, 0.8441, 0.2899, 0.1316, 0.1317, 0.2455,\n",
      "        0.7458, 0.2349, 0.7364, 0.1779, 0.7077, 0.8296, 0.2466, 0.8218, 0.2612,\n",
      "        0.2768, 0.8481, 0.6309, 0.6011, 0.2466, 0.7727, 0.8679, 0.7387, 0.3202,\n",
      "        0.2570, 0.3299, 0.7736, 0.7842, 0.7222, 0.2100, 0.2977, 0.1710, 0.7070,\n",
      "        0.8528, 0.2275, 0.8456, 0.3112, 0.2008, 0.8192, 0.6420, 0.4040, 0.1464,\n",
      "        0.7985, 0.8748, 0.2482, 0.8432, 0.2085]), tensor([0.6567, 0.7892, 0.8056, 0.8456, 0.8812, 0.1976, 0.1495, 0.2143, 0.7867,\n",
      "        0.1350, 0.1746, 0.5283, 0.7631, 0.8553, 0.3102, 0.7691, 0.6722, 0.8035,\n",
      "        0.2073, 0.8070, 0.1475, 0.1458, 0.4435, 0.8258, 0.6741, 0.8043, 0.3181,\n",
      "        0.7801, 0.1853, 0.9035, 0.4516, 0.2364, 0.8106, 0.8168, 0.7902, 0.1849,\n",
      "        0.3491, 0.8394, 0.7922, 0.8787, 0.1751, 0.8503, 0.8268, 0.8557, 0.1306,\n",
      "        0.2210, 0.8592, 0.7650, 0.8134, 0.5647]), tensor([0.9025, 0.2415, 0.1414, 0.8308, 0.2504, 0.2861, 0.8631, 0.1449, 0.1192,\n",
      "        0.7881, 0.7487, 0.1403, 0.1720, 0.3882, 0.4320, 0.2489, 0.8453, 0.1734,\n",
      "        0.7860, 0.8777, 0.1800, 0.1184, 0.8381, 0.8204, 0.8494, 0.7807, 0.6518,\n",
      "        0.8570, 0.8482, 0.6746, 0.2671, 0.8761, 0.7225, 0.8568, 0.7838, 0.2116,\n",
      "        0.1855, 0.8547, 0.8385, 0.6876, 0.1633, 0.5046, 0.1914, 0.2401, 0.8480,\n",
      "        0.1590, 0.1919, 0.3147, 0.4598, 0.1592]), tensor([0.8750, 0.1911, 0.3296, 0.3714, 0.2455, 0.3534, 0.1876, 0.1754, 0.8246,\n",
      "        0.7288, 0.8366, 0.7514, 0.1467, 0.2260, 0.7806, 0.8494, 0.7923, 0.1754,\n",
      "        0.7975, 0.8691, 0.7996, 0.8541, 0.1858, 0.1716, 0.8436, 0.1948, 0.8678,\n",
      "        0.7787, 0.1795, 0.7920, 0.8286, 0.2008, 0.7817, 0.4926, 0.7039, 0.1518,\n",
      "        0.8053, 0.8468, 0.8770, 0.7658, 0.1971, 0.8187, 0.3282, 0.8135, 0.8469,\n",
      "        0.1759, 0.2017, 0.8569, 0.1538, 0.1534]), tensor([0.7012, 0.1803, 0.7588, 0.7480, 0.1549, 0.1691, 0.5212, 0.2361, 0.2720,\n",
      "        0.1337, 0.3053, 0.1857, 0.8714, 0.4715, 0.8761, 0.8519, 0.1961, 0.1987,\n",
      "        0.5345, 0.1625, 0.2445, 0.6917, 0.5790, 0.1833, 0.8557, 0.8775, 0.8228,\n",
      "        0.2529, 0.1902, 0.1689, 0.8118, 0.7632, 0.8528, 0.8712, 0.8490, 0.7691,\n",
      "        0.1391, 0.5050, 0.5400, 0.8255, 0.7549, 0.8845, 0.2964, 0.7182, 0.1577,\n",
      "        0.1715, 0.1662, 0.2268, 0.3496, 0.6051]), tensor([0.6857, 0.7179, 0.8076, 0.7669, 0.8863, 0.1412, 0.1426, 0.8361, 0.2107,\n",
      "        0.1731, 0.6497, 0.1599, 0.8635, 0.2123, 0.4477, 0.2004, 0.6546, 0.2199,\n",
      "        0.8096, 0.3746, 0.8174, 0.3576, 0.7856, 0.7525, 0.6602, 0.2771, 0.4115,\n",
      "        0.2667, 0.2031, 0.5554, 0.6131, 0.7934, 0.7581, 0.7666, 0.8574, 0.0970,\n",
      "        0.8682, 0.6850, 0.7791, 0.8622, 0.8486, 0.8271, 0.7982, 0.7634, 0.6842,\n",
      "        0.8177, 0.1765, 0.8145, 0.2294, 0.3376]), tensor([0.3070, 0.2728, 0.8853, 0.8566, 0.7998, 0.8723, 0.8400, 0.7856, 0.1301,\n",
      "        0.4168, 0.8230, 0.2111, 0.8498, 0.3876, 0.7886, 0.2323, 0.2165, 0.1420,\n",
      "        0.8303, 0.2793, 0.1644, 0.7862, 0.1250, 0.7959, 0.2454, 0.8664, 0.8242,\n",
      "        0.8823, 0.9018, 0.2110, 0.8603, 0.8315, 0.6312, 0.8573, 0.8196, 0.8088,\n",
      "        0.8062, 0.8475, 0.8608, 0.8162, 0.7711, 0.7924, 0.8656, 0.8309, 0.7639,\n",
      "        0.7338, 0.1593, 0.1490, 0.8411, 0.8812]), tensor([0.1643, 0.8676, 0.8272, 0.1654, 0.8927, 0.1983, 0.6195, 0.4980, 0.8652,\n",
      "        0.5253, 0.7707, 0.3252, 0.8394, 0.1493, 0.8602, 0.8698, 0.8921, 0.2263,\n",
      "        0.4546, 0.2282, 0.7806, 0.6267, 0.8557, 0.8256, 0.2344, 0.6355, 0.7931,\n",
      "        0.8563, 0.8373, 0.8633, 0.2491, 0.2358, 0.8852, 0.8253, 0.7862, 0.8867,\n",
      "        0.1571, 0.8260, 0.8027, 0.2837, 0.8394, 0.1841, 0.6801, 0.1781, 0.1496,\n",
      "        0.3886, 0.1677, 0.1558, 0.6984, 0.8692]), tensor([0.1699, 0.2434, 0.8494, 0.1917, 0.7518, 0.7939, 0.8470, 0.8198, 0.7706,\n",
      "        0.1580, 0.2261, 0.8704, 0.8234, 0.7976, 0.8632, 0.2785, 0.1643, 0.8570,\n",
      "        0.8071, 0.8068, 0.7954, 0.1563, 0.8213, 0.8430, 0.7279, 0.7915, 0.4573,\n",
      "        0.8201, 0.3997, 0.7389, 0.3454, 0.1826, 0.7540, 0.7429, 0.0942, 0.2021,\n",
      "        0.1391, 0.6272, 0.8517, 0.7554, 0.8811, 0.7394, 0.7900, 0.2318, 0.4942,\n",
      "        0.7172, 0.1621, 0.1423, 0.2043, 0.8420]), tensor([0.2538, 0.6546, 0.8292, 0.8401, 0.8811, 0.8816, 0.2042, 0.6149, 0.7846,\n",
      "        0.5676, 0.8671, 0.1533, 0.5079, 0.1807, 0.5562, 0.2949, 0.1760, 0.4123,\n",
      "        0.7544, 0.3870, 0.1177, 0.1797, 0.7512, 0.8709, 0.1586, 0.8572, 0.1997,\n",
      "        0.7865, 0.8221, 0.4356, 0.2163, 0.7404, 0.7360, 0.2214, 0.8757, 0.1632,\n",
      "        0.2387, 0.2900, 0.8623, 0.6689, 0.2202, 0.6559, 0.2928, 0.4759, 0.5825,\n",
      "        0.8940, 0.8639, 0.7663, 0.5690, 0.1365]), tensor([0.1442, 0.8330, 0.8352, 0.1555, 0.8098, 0.7604, 0.2169, 0.8266, 0.1929,\n",
      "        0.8445, 0.8964, 0.2360, 0.2479, 0.3543, 0.8614, 0.1647, 0.5955, 0.1651,\n",
      "        0.2101, 0.7806, 0.8119, 0.6615, 0.8103, 0.8654, 0.1898, 0.8617, 0.8542,\n",
      "        0.8429, 0.8071, 0.7788, 0.7351, 0.1645, 0.8369, 0.7355, 0.3113, 0.8459,\n",
      "        0.8118, 0.3112, 0.8357, 0.6146, 0.2786, 0.2738, 0.2114, 0.1292, 0.1897,\n",
      "        0.8607, 0.8275, 0.8093, 0.7478, 0.1753]), tensor([0.8004, 0.8553, 0.6916, 0.8800, 0.8254, 0.8506, 0.2829, 0.1678, 0.7916,\n",
      "        0.8128, 0.8377, 0.7244, 0.4609, 0.1870, 0.1390, 0.8243, 0.2947, 0.8232,\n",
      "        0.8439, 0.3556, 0.8936, 0.1748, 0.1254, 0.1791, 0.7483, 0.8255, 0.8459,\n",
      "        0.3205, 0.7453, 0.2921, 0.8666, 0.1062, 0.1746, 0.1766, 0.7591, 0.7805,\n",
      "        0.8809, 0.2364, 0.8346, 0.8059, 0.8805, 0.8524, 0.8055, 0.8660, 0.8593,\n",
      "        0.1622, 0.8335, 0.1908, 0.2930, 0.4326]), tensor([0.8400, 0.3234, 0.1871, 0.7080, 0.7976, 0.8051, 0.1451, 0.1320, 0.7898,\n",
      "        0.5136, 0.7717, 0.1549, 0.4501, 0.2086, 0.5259, 0.6068, 0.2427, 0.8636,\n",
      "        0.8589, 0.8057, 0.4685, 0.8458, 0.6530, 0.3490, 0.2525, 0.8318, 0.1681,\n",
      "        0.2014, 0.8242, 0.8020, 0.6400, 0.8480, 0.1974, 0.6551, 0.7089, 0.8069,\n",
      "        0.8558, 0.5287, 0.4794, 0.8678, 0.1449, 0.4821, 0.5257, 0.8562, 0.3706,\n",
      "        0.8892, 0.4088, 0.8896, 0.7982, 0.1540]), tensor([0.8657, 0.8795, 0.8470, 0.8772, 0.8730, 0.1368, 0.1518, 0.6370, 0.8424,\n",
      "        0.8513, 0.2523, 0.7898, 0.8058, 0.8174, 0.8610, 0.7438, 0.1629, 0.3414,\n",
      "        0.8440, 0.7769, 0.7854, 0.5874, 0.6580, 0.5178, 0.2454, 0.7702, 0.1535,\n",
      "        0.5233, 0.7686, 0.7944, 0.8995, 0.1435, 0.4614, 0.4759, 0.2173, 0.8317,\n",
      "        0.2708, 0.8352, 0.1848, 0.5784, 0.7527, 0.7659, 0.5746, 0.8267, 0.1537,\n",
      "        0.1434, 0.8141, 0.0968, 0.7527, 0.8063]), tensor([0.1795, 0.5127, 0.8222, 0.8451, 0.7118, 0.2815, 0.5066, 0.2037, 0.2577,\n",
      "        0.8374, 0.8405, 0.7896, 0.7974, 0.3045, 0.1435, 0.7337, 0.3014, 0.4872,\n",
      "        0.7766, 0.8298, 0.2914, 0.8417, 0.6349, 0.5629, 0.3503, 0.8083, 0.7873,\n",
      "        0.1243, 0.8760, 0.7571, 0.7780, 0.7206, 0.1962, 0.8298, 0.8046, 0.7179,\n",
      "        0.8424, 0.8545, 0.8208, 0.7963, 0.7604, 0.8287, 0.2087, 0.5320, 0.8224,\n",
      "        0.7149, 0.8128, 0.1649, 0.8342, 0.7803]), tensor([0.2632, 0.5325, 0.5981, 0.7727, 0.7916, 0.1392, 0.7512, 0.7729, 0.1509,\n",
      "        0.1747, 0.5277, 0.2609, 0.5399, 0.1686, 0.7752, 0.5183, 0.7190, 0.0910,\n",
      "        0.7686, 0.7342, 0.8135, 0.3671, 0.8126, 0.8212, 0.3880, 0.2178, 0.2431,\n",
      "        0.2590, 0.2646, 0.2113, 0.8320, 0.1875, 0.1297, 0.6301, 0.7947, 0.8498,\n",
      "        0.4021, 0.2552, 0.3688, 0.6610, 0.3416, 0.2383, 0.1686, 0.1962, 0.8775,\n",
      "        0.7906, 0.8685, 0.8395, 0.8581, 0.9009]), tensor([0.8172, 0.3631, 0.1987, 0.1549, 0.8586, 0.1355, 0.7557, 0.7985, 0.1747,\n",
      "        0.8110, 0.8608, 0.8286, 0.8796, 0.8667, 0.8572, 0.8848, 0.8341, 0.2884,\n",
      "        0.4655, 0.8503, 0.7487, 0.8612, 0.4502, 0.8377, 0.7133, 0.1430, 0.8157,\n",
      "        0.3257, 0.8747, 0.5910, 0.5842, 0.8594, 0.8899, 0.8532, 0.1630, 0.7472,\n",
      "        0.3121, 0.8203, 0.1616, 0.1529, 0.1289, 0.6972, 0.3479, 0.5915, 0.1862,\n",
      "        0.8642, 0.8500, 0.7655, 0.5985, 0.6542]), tensor([0.8454, 0.8389, 0.2847, 0.2605, 0.8705, 0.2716, 0.7748, 0.2128, 0.7835,\n",
      "        0.4906, 0.6292, 0.8797, 0.8742, 0.5477, 0.8798, 0.7710, 0.8658, 0.2354,\n",
      "        0.1893, 0.8451, 0.8386, 0.7571, 0.8129, 0.1491, 0.8576, 0.6981, 0.1642,\n",
      "        0.4038, 0.4233, 0.8498, 0.4509, 0.4412, 0.7803, 0.2174, 0.7558, 0.8463,\n",
      "        0.7712, 0.1486, 0.7901, 0.7304, 0.1317, 0.2019, 0.8247, 0.1722, 0.1543,\n",
      "        0.7709, 0.8011, 0.6479, 0.5631, 0.6199]), tensor([0.5622, 0.2028, 0.5666, 0.1936, 0.5127, 0.1225, 0.1328, 0.8124, 0.2207,\n",
      "        0.1755, 0.8291, 0.2060, 0.5675, 0.7843, 0.2767, 0.8185, 0.8307, 0.8523,\n",
      "        0.6538, 0.9034, 0.8787, 0.1496, 0.4713, 0.1637, 0.7611, 0.7400, 0.8029,\n",
      "        0.8547, 0.8652, 0.1882, 0.8589, 0.1659, 0.3511, 0.7337, 0.3285, 0.3165,\n",
      "        0.8673, 0.8792, 0.8581, 0.8805, 0.1476, 0.3419, 0.8409, 0.7243, 0.8556,\n",
      "        0.6438, 0.8586, 0.8240, 0.2087, 0.8341]), tensor([0.5583, 0.1619, 0.8944, 0.7407, 0.3322, 0.1390, 0.2656, 0.1684, 0.1705,\n",
      "        0.2939, 0.1543, 0.2307, 0.1627, 0.2442, 0.8209, 0.8110, 0.5327, 0.1776,\n",
      "        0.3779, 0.3200, 0.7860, 0.1640, 0.2461, 0.1796, 0.1394, 0.2009, 0.7657,\n",
      "        0.7087, 0.8219, 0.7419, 0.8422, 0.7459, 0.5896, 0.3284, 0.2183, 0.2738,\n",
      "        0.5246, 0.3462, 0.2311, 0.1541, 0.1502, 0.7825, 0.8239, 0.3197, 0.7996,\n",
      "        0.3693, 0.7151, 0.1783, 0.1450, 0.8535]), tensor([0.8867, 0.1940, 0.8240, 0.6496, 0.5022, 0.7722, 0.1809, 0.7997, 0.3974,\n",
      "        0.8909, 0.1334, 0.1878, 0.8039, 0.1514, 0.8060, 0.8505, 0.3126, 0.6702,\n",
      "        0.8565, 0.1438, 0.6534, 0.1744, 0.8292, 0.2196, 0.1421, 0.1779, 0.8037,\n",
      "        0.8384, 0.4206, 0.7404, 0.4802, 0.7503, 0.7028, 0.2313, 0.8862, 0.4966,\n",
      "        0.8382, 0.7600, 0.1848, 0.8801, 0.6910, 0.2064, 0.8296, 0.2539, 0.4242,\n",
      "        0.2115, 0.8338, 0.6088, 0.3095, 0.1636]), tensor([0.8037, 0.1680, 0.1890, 0.7798, 0.8718, 0.8331, 0.8003, 0.8509, 0.6038,\n",
      "        0.8200, 0.1580, 0.1853, 0.8480, 0.2804, 0.1316, 0.2023, 0.2465, 0.8051,\n",
      "        0.7345, 0.9090, 0.8839, 0.8554, 0.8106, 0.1735, 0.2604, 0.8665, 0.8640,\n",
      "        0.8613, 0.8572, 0.7942, 0.8323, 0.2974, 0.8596, 0.8174, 0.8641, 0.8823,\n",
      "        0.2750, 0.6922, 0.8145, 0.5527, 0.3296, 0.2588, 0.8183, 0.8672, 0.7746,\n",
      "        0.8588, 0.1373, 0.6225, 0.8665, 0.5430]), tensor([0.1062, 0.1184, 0.2053, 0.8246, 0.3322, 0.3928, 0.8867, 0.8842, 0.8774,\n",
      "        0.7609, 0.8171, 0.8216, 0.4243, 0.1672, 0.1994, 0.2276, 0.2760, 0.5095,\n",
      "        0.7076, 0.7924, 0.3583, 0.8331, 0.1159, 0.8304, 0.6991, 0.8161, 0.8680,\n",
      "        0.2339, 0.2624, 0.8575, 0.7894, 0.8710, 0.1788, 0.3029, 0.8629, 0.2929,\n",
      "        0.3702, 0.1209, 0.8204, 0.2269, 0.7920, 0.1174, 0.8346, 0.8586, 0.6316,\n",
      "        0.2857, 0.1319, 0.8071, 0.1530, 0.2648]), tensor([0.7476, 0.8574, 0.1660, 0.2101, 0.8412, 0.8481, 0.7934, 0.8216, 0.7821,\n",
      "        0.8071, 0.2306, 0.7313, 0.1490, 0.1950, 0.1978, 0.8589, 0.8701, 0.5209,\n",
      "        0.2357, 0.8217, 0.1536, 0.1935, 0.1512, 0.3838, 0.1486, 0.4989, 0.7653,\n",
      "        0.7632, 0.5156, 0.3183, 0.5055, 0.8006, 0.1468, 0.2544, 0.8328, 0.1486,\n",
      "        0.8697, 0.8629, 0.7940, 0.8732, 0.1396, 0.7921, 0.8689, 0.3862, 0.1108,\n",
      "        0.1318, 0.1450, 0.8494, 0.3710, 0.1552]), tensor([0.8126, 0.8466, 0.7751, 0.1426, 0.8735, 0.7910, 0.1326, 0.2048, 0.1448,\n",
      "        0.5209, 0.2124, 0.3349, 0.7248, 0.7978, 0.1701, 0.5840, 0.8725, 0.1867,\n",
      "        0.8181, 0.5284, 0.8363, 0.1412, 0.3079, 0.8624, 0.1715, 0.1340, 0.7065,\n",
      "        0.7173, 0.7182, 0.8217, 0.2240, 0.2986, 0.8554, 0.1807, 0.4009, 0.7164,\n",
      "        0.8367, 0.7494, 0.7350, 0.6238, 0.8479, 0.1709, 0.8327, 0.1977, 0.1639,\n",
      "        0.6969, 0.1491, 0.8163, 0.8431, 0.8166]), tensor([0.7729, 0.2053, 0.7446, 0.8030, 0.2297, 0.7897, 0.1507, 0.1778, 0.2562,\n",
      "        0.1564, 0.7587, 0.5314, 0.7935, 0.8785, 0.7244, 0.1910, 0.1773, 0.2574,\n",
      "        0.8131, 0.8370, 0.8344, 0.2038, 0.1669, 0.2017, 0.4292, 0.1940, 0.6207,\n",
      "        0.1628, 0.1908, 0.6902, 0.1328, 0.8185, 0.3598, 0.8293, 0.8501, 0.1860,\n",
      "        0.2687, 0.4829, 0.1985, 0.8622, 0.2757, 0.8217, 0.7757, 0.6657, 0.8236,\n",
      "        0.7557, 0.7861, 0.8050, 0.8578, 0.8239]), tensor([0.1410, 0.2128, 0.7657, 0.8746, 0.1970, 0.4877, 0.1892, 0.1756, 0.2212,\n",
      "        0.2753, 0.4059, 0.6485, 0.6661, 0.8326, 0.8088, 0.8791, 0.1558, 0.8000,\n",
      "        0.8543, 0.5964, 0.2628, 0.2365, 0.8345, 0.4039, 0.1975, 0.2719, 0.8882,\n",
      "        0.7820, 0.1196, 0.8173, 0.1225, 0.5856, 0.7398, 0.1699, 0.8537, 0.4616,\n",
      "        0.8047, 0.8320, 0.7418, 0.8304, 0.6302, 0.1339, 0.8202, 0.8677, 0.6642,\n",
      "        0.6401, 0.8060, 0.1606, 0.8154, 0.7742]), tensor([0.6945, 0.1802, 0.8664, 0.2313, 0.7641, 0.7284, 0.7800, 0.6315, 0.6588,\n",
      "        0.1867, 0.1599, 0.6939, 0.5293, 0.7961, 0.1450, 0.8875, 0.1557, 0.1751,\n",
      "        0.5349, 0.8626, 0.7844, 0.4196, 0.6564, 0.8379, 0.6948, 0.1278, 0.8452,\n",
      "        0.7570, 0.8161, 0.8003, 0.1675, 0.8377, 0.1542, 0.8330, 0.2106, 0.2142,\n",
      "        0.8225, 0.7912, 0.7285, 0.2367, 0.8120, 0.2436, 0.6097, 0.5061, 0.2973,\n",
      "        0.1913, 0.1680, 0.2427, 0.1458, 0.1715]), tensor([0.1489, 0.1646, 0.4917, 0.1411, 0.1504, 0.1373, 0.2567, 0.1571, 0.1929,\n",
      "        0.8942, 0.1414, 0.7041, 0.8692, 0.8736, 0.3730, 0.8270, 0.2165, 0.8143,\n",
      "        0.1879, 0.8803, 0.6399, 0.7104, 0.2034, 0.7803, 0.6787, 0.8220, 0.2653,\n",
      "        0.6816, 0.7717, 0.8038, 0.1707, 0.4625, 0.3220, 0.2112, 0.8194, 0.1983,\n",
      "        0.7746, 0.2372, 0.1387, 0.2911, 0.7218, 0.7934, 0.2894, 0.4355, 0.6097,\n",
      "        0.1273, 0.8733, 0.8375, 0.6410, 0.8667]), tensor([0.8392, 0.8179, 0.2496, 0.3739, 0.6507, 0.4305, 0.7097, 0.2459, 0.8005,\n",
      "        0.8484, 0.8063, 0.7054, 0.6431, 0.8693, 0.1883, 0.8462, 0.1630, 0.8017,\n",
      "        0.6765, 0.1376, 0.8426, 0.1171, 0.5819, 0.8748, 0.8024, 0.8825, 0.8610,\n",
      "        0.7399, 0.7770, 0.8001, 0.8738, 0.8201, 0.2199, 0.8419, 0.8287, 0.3172,\n",
      "        0.6706, 0.7156, 0.1909, 0.1468, 0.2479, 0.1988, 0.8122, 0.7969, 0.8171,\n",
      "        0.7730, 0.3218, 0.8440, 0.2275, 0.1253]), tensor([0.1141, 0.8941, 0.8443, 0.2097, 0.8335, 0.8441, 0.4893, 0.7877, 0.8137,\n",
      "        0.7963, 0.2312, 0.4676, 0.3951, 0.1965, 0.8438, 0.6882, 0.2801, 0.8378,\n",
      "        0.7431, 0.1935, 0.1703, 0.1020, 0.7573, 0.4213, 0.7521, 0.2539, 0.8324,\n",
      "        0.6380, 0.2219, 0.1532, 0.7260, 0.2003, 0.8336, 0.2054, 0.2095, 0.6088,\n",
      "        0.1962, 0.4365, 0.1686, 0.7785, 0.8265, 0.8929, 0.1627, 0.7747, 0.8000,\n",
      "        0.1874, 0.2870, 0.8630, 0.1272, 0.1952]), tensor([0.8500, 0.3575, 0.2745, 0.3151, 0.2757, 0.8494, 0.7804, 0.7940, 0.7759,\n",
      "        0.8329, 0.1518, 0.8898, 0.8279, 0.8872, 0.2181, 0.8267, 0.8456, 0.2159,\n",
      "        0.8086, 0.5091, 0.6707, 0.8872, 0.8698, 0.8117, 0.6571, 0.8847, 0.5803,\n",
      "        0.6133, 0.8635, 0.8302, 0.8557, 0.8379, 0.2171, 0.1556, 0.9075, 0.8769,\n",
      "        0.7618, 0.8511, 0.8574, 0.4463, 0.5259, 0.4573, 0.2865, 0.4282, 0.1825,\n",
      "        0.1551, 0.3056, 0.4860, 0.1624, 0.6684]), tensor([0.4063, 0.8273, 0.6924, 0.8690, 0.6262, 0.8738, 0.8851, 0.8615, 0.7241,\n",
      "        0.8589, 0.1642, 0.2573, 0.2304, 0.7653, 0.8728, 0.7923, 0.6284, 0.8201,\n",
      "        0.5185, 0.1794, 0.6167, 0.2012, 0.1385, 0.8815, 0.8020, 0.2438, 0.7638,\n",
      "        0.2641, 0.1706, 0.7707, 0.7830, 0.1063, 0.1305, 0.1604, 0.1815, 0.2898,\n",
      "        0.8623, 0.2654, 0.8273, 0.8428, 0.2023, 0.4159, 0.7062, 0.7879, 0.8308,\n",
      "        0.3474, 0.6389, 0.2100, 0.8308, 0.7619]), tensor([0.8401, 0.4524, 0.2500, 0.7585, 0.7650, 0.6862, 0.4841, 0.5449, 0.8119,\n",
      "        0.1750, 0.7484, 0.5207, 0.6465, 0.8541, 0.2839, 0.8054, 0.8502, 0.2570,\n",
      "        0.4811, 0.8249, 0.7804, 0.8303, 0.2188, 0.8076, 0.8523, 0.8164, 0.1324,\n",
      "        0.8781, 0.2792, 0.8434, 0.8252, 0.8177, 0.4379, 0.2362, 0.8705, 0.3351,\n",
      "        0.1777, 0.2316, 0.8346, 0.8397, 0.8272, 0.1635, 0.2422, 0.8235, 0.8310,\n",
      "        0.7169, 0.3685, 0.1414, 0.1600, 0.1717]), tensor([0.2367, 0.3052, 0.8584, 0.8501, 0.8100, 0.8745, 0.2253, 0.2155, 0.2080,\n",
      "        0.2271, 0.8353, 0.6543, 0.8448, 0.7751, 0.7945, 0.8519, 0.8527, 0.8131,\n",
      "        0.2783, 0.1641, 0.3833, 0.7343, 0.4132, 0.7761, 0.8809, 0.7693, 0.2596,\n",
      "        0.7108, 0.4597, 0.7606, 0.8664, 0.7970, 0.8826, 0.8826, 0.3371, 0.8010,\n",
      "        0.3573, 0.2424, 0.6927, 0.7969, 0.1558, 0.7403, 0.7737, 0.8938, 0.4943,\n",
      "        0.8149, 0.7844, 0.7646, 0.2873, 0.5230]), tensor([0.1640, 0.1715, 0.8897, 0.8241, 0.3120, 0.6576, 0.8382, 0.2433, 0.3164,\n",
      "        0.7771, 0.1755, 0.5255, 0.2244, 0.8465, 0.7068, 0.6428, 0.3931, 0.1314,\n",
      "        0.7794, 0.8608, 0.2339, 0.6169, 0.8731, 0.7676, 0.1188, 0.1733, 0.2121,\n",
      "        0.1363, 0.4860, 0.1611, 0.2431, 0.7888, 0.4127, 0.8236, 0.1775, 0.8523,\n",
      "        0.2658, 0.1367, 0.2185, 0.5424, 0.8154, 0.3000, 0.1627, 0.8445, 0.8360,\n",
      "        0.1563, 0.7671, 0.7906, 0.2919, 0.1824]), tensor([0.7964, 0.8686, 0.8659, 0.8027, 0.1540, 0.1766, 0.7967, 0.3213, 0.3710,\n",
      "        0.8629, 0.2833, 0.4928, 0.7426, 0.2155, 0.7678, 0.7612, 0.6854, 0.7564,\n",
      "        0.8367, 0.7120, 0.6645, 0.7625, 0.1663, 0.8229, 0.1964, 0.3105, 0.7491,\n",
      "        0.8286, 0.6851, 0.1588, 0.8198, 0.3252, 0.8280, 0.6625, 0.1320, 0.3030,\n",
      "        0.8490, 0.8117, 0.3093, 0.8007, 0.2218, 0.7980, 0.8716, 0.1567, 0.2755,\n",
      "        0.3843, 0.1576, 0.7666, 0.8184, 0.8956]), tensor([0.8204, 0.1558, 0.1760, 0.6539, 0.1561, 0.1395, 0.8122, 0.7094, 0.5145,\n",
      "        0.1498, 0.8693, 0.8889, 0.5069, 0.1541, 0.1633, 0.2126, 0.1857, 0.8834,\n",
      "        0.8452, 0.2922, 0.1318, 0.8681, 0.7326, 0.7291, 0.7825, 0.1401, 0.1879,\n",
      "        0.4126, 0.3696, 0.2182, 0.8707, 0.8223, 0.1821, 0.6035, 0.1861, 0.2092,\n",
      "        0.1608, 0.8500, 0.2346, 0.5067, 0.3431, 0.6309, 0.8369, 0.8542, 0.1650,\n",
      "        0.7896, 0.8463, 0.4111, 0.1966, 0.1380]), tensor([0.7781, 0.6863, 0.7971, 0.1536, 0.2055, 0.8870, 0.8669, 0.8910, 0.2916,\n",
      "        0.1506, 0.4348, 0.7724, 0.7784, 0.1731, 0.7543, 0.8233, 0.1606, 0.8266,\n",
      "        0.1578, 0.8106, 0.3920, 0.6337, 0.7218, 0.8489, 0.1901, 0.2400, 0.1877,\n",
      "        0.1718, 0.1773, 0.2008, 0.7620, 0.1676, 0.7897, 0.2525, 0.1524, 0.8956,\n",
      "        0.7701, 0.7133, 0.8150, 0.8135, 0.1334, 0.8738, 0.2049, 0.8942, 0.1427,\n",
      "        0.6358, 0.8444, 0.6500, 0.2043, 0.3206]), tensor([0.1581, 0.2613, 0.8650, 0.1842, 0.1356, 0.8810, 0.1667, 0.3220, 0.1764,\n",
      "        0.1932, 0.8320, 0.7919, 0.7901, 0.8908, 0.2109, 0.7933, 0.7313, 0.3013,\n",
      "        0.5801, 0.1633, 0.7987, 0.4478, 0.2941, 0.2274, 0.8553, 0.2201, 0.8289,\n",
      "        0.8844, 0.8320, 0.1920, 0.8397, 0.1377, 0.1543, 0.8691, 0.1478, 0.7393,\n",
      "        0.8141, 0.3515, 0.1951, 0.1653, 0.8681, 0.8395, 0.8688, 0.8170, 0.8056,\n",
      "        0.6322, 0.1590, 0.2327, 0.6543, 0.1556]), tensor([0.8523, 0.7765, 0.2040, 0.1372, 0.2797, 0.1478, 0.1215, 0.6550, 0.4173,\n",
      "        0.1669, 0.8784, 0.2041, 0.8433, 0.2325, 0.1170, 0.2497, 0.1742, 0.1495,\n",
      "        0.3912, 0.1733, 0.1889, 0.4310, 0.1643, 0.1510, 0.8019, 0.1850, 0.1780,\n",
      "        0.7944, 0.8359, 0.8653, 0.8469, 0.1892, 0.2611, 0.8788, 0.8505, 0.7998,\n",
      "        0.8438, 0.1551, 0.2377, 0.1915, 0.1908, 0.2672, 0.8190, 0.2980, 0.1697,\n",
      "        0.7973, 0.3927, 0.1550, 0.1364, 0.2653]), tensor([0.7960, 0.6946, 0.1699, 0.8599, 0.3905, 0.4622, 0.2703, 0.6928, 0.7770,\n",
      "        0.7821, 0.1789, 0.2682, 0.7893, 0.2800, 0.1646, 0.4707, 0.8157, 0.1343,\n",
      "        0.6598, 0.7406, 0.7907, 0.2013, 0.4358, 0.5661, 0.8482, 0.1678, 0.1515,\n",
      "        0.7001, 0.8495, 0.7959, 0.7110, 0.8541, 0.2895, 0.1764, 0.1606, 0.8760,\n",
      "        0.6216, 0.6024, 0.8374, 0.8438, 0.3246, 0.7254, 0.1760, 0.1554, 0.7870,\n",
      "        0.1517, 0.8191, 0.3571, 0.8726, 0.2674]), tensor([0.8293, 0.6757, 0.6979, 0.8059, 0.7884, 0.8642, 0.8005, 0.1887, 0.5795,\n",
      "        0.1236, 0.7940, 0.8660, 0.8971, 0.1433, 0.1784, 0.8356, 0.1572, 0.7218,\n",
      "        0.8714, 0.4551, 0.8468, 0.1603, 0.3863, 0.7792, 0.8420, 0.3408, 0.2221,\n",
      "        0.7898, 0.1707, 0.8359, 0.8506, 0.2336, 0.8406, 0.2245, 0.3273, 0.8109,\n",
      "        0.8684, 0.8274, 0.7687, 0.7509, 0.6973, 0.1338, 0.8157, 0.1887, 0.1425,\n",
      "        0.1059, 0.2505, 0.7803, 0.8651, 0.8481]), tensor([0.2781, 0.4975, 0.2244, 0.8302, 0.4321, 0.4061, 0.7337, 0.6941, 0.7481,\n",
      "        0.7957, 0.2781, 0.3460, 0.1602, 0.2879, 0.2457, 0.7892, 0.7470, 0.2273,\n",
      "        0.7991, 0.7182, 0.2920, 0.1879, 0.7927, 0.1732, 0.7948, 0.4569, 0.6089,\n",
      "        0.1933, 0.8555, 0.7329, 0.7967, 0.2041, 0.8372, 0.7783, 0.8654, 0.7860,\n",
      "        0.9070, 0.1808, 0.8592, 0.8759, 0.8698, 0.1651, 0.7570, 0.8440, 0.7961,\n",
      "        0.7780, 0.1934, 0.2288, 0.8160, 0.8453]), tensor([0.6296, 0.8567, 0.4712, 0.8299, 0.8762, 0.1788, 0.1662, 0.8526, 0.8383,\n",
      "        0.1785, 0.2571, 0.7277, 0.8375, 0.6575, 0.3741, 0.1616, 0.2304, 0.8070,\n",
      "        0.1587, 0.1852, 0.7795, 0.7794, 0.8366, 0.8222, 0.8618, 0.8119, 0.8650,\n",
      "        0.2127, 0.5776, 0.1754, 0.8273, 0.8519, 0.8863, 0.8755, 0.3842, 0.6152,\n",
      "        0.1934, 0.2663, 0.1332, 0.2336, 0.8135, 0.7874, 0.7853, 0.7696, 0.8519,\n",
      "        0.2899, 0.2122, 0.2266, 0.2042, 0.6534])]\n"
     ]
    }
   ],
   "source": [
    "prediction = predictions(reviews)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1194</td>\n",
       "      <td>attempt line i let the nuclear scene beautiful...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1128</td>\n",
       "      <td>self from with could socially least in the old...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1281</td>\n",
       "      <td>gambling for about jungle one the plays have t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>879</td>\n",
       "      <td>cancelled be slashers trying wasn all go more ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>729</td>\n",
       "      <td>semester the think but lacks creek of it of go...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1652</td>\n",
       "      <td>be for chemistry explicit more is so would to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>783</td>\n",
       "      <td>the work me about one seizures for of film ave...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2476</td>\n",
       "      <td>etc tunnel many as wasted hordes california vi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2390</td>\n",
       "      <td>the that loved edgy or on seen glorious are su...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>985</td>\n",
       "      <td>voice who production be car the the mountain h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1738</td>\n",
       "      <td>with jewelry joe why good who bath intriguing ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>808</td>\n",
       "      <td>the secret the second the with cure we think o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1550</td>\n",
       "      <td>the time isn after the second is sequences the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1530</td>\n",
       "      <td>reach morphs new re turn scenes this to the it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>69</td>\n",
       "      <td>impossible philip him haven unrelated what on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>527</td>\n",
       "      <td>think but be to sex out at good like his was w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1207</td>\n",
       "      <td>film have stupid long if of so no one the true...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2404</td>\n",
       "      <td>the war nicole nobody of very car with contemp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2435</td>\n",
       "      <td>one the everyone traditional if or lacks event...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>290</td>\n",
       "      <td>nicely that also so finds pretty despite final...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                             review  sentiment\n",
       "0    1194  attempt line i let the nuclear scene beautiful...          0\n",
       "1    1128  self from with could socially least in the old...          0\n",
       "2    1281  gambling for about jungle one the plays have t...          0\n",
       "3     879  cancelled be slashers trying wasn all go more ...          0\n",
       "4     729  semester the think but lacks creek of it of go...          0\n",
       "5    1652  be for chemistry explicit more is so would to ...          0\n",
       "6     783  the work me about one seizures for of film ave...          0\n",
       "7    2476  etc tunnel many as wasted hordes california vi...          1\n",
       "8    2390  the that loved edgy or on seen glorious are su...          1\n",
       "9     985  voice who production be car the the mountain h...          0\n",
       "10   1738  with jewelry joe why good who bath intriguing ...          1\n",
       "11    808  the secret the second the with cure we think o...          0\n",
       "12   1550  the time isn after the second is sequences the...          0\n",
       "13   1530  reach morphs new re turn scenes this to the it...          0\n",
       "14     69  impossible philip him haven unrelated what on ...          0\n",
       "15    527  think but be to sex out at good like his was w...          0\n",
       "16   1207  film have stupid long if of so no one the true...          0\n",
       "17   2404  the war nicole nobody of very car with contemp...          1\n",
       "18   2435  one the everyone traditional if or lacks event...          1\n",
       "19    290  nicely that also so finds pretty despite final...          1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'etc tunnel many as wasted hordes california violence and proposal one disposal into of the way of worth regret master and and the let running not refresh for the of time should right more horror seen another this is accountant the seeing settled is then there attempts that discarded mastery blade sophisticated sammi last this in film dark enough daring eye the the the the are delinquency he and and and how by other this charles about expression enjoyment hate work and may status us recommended even treat and bold video that green warning wonderful end yet that a hughes their having title movie criminals kate the in anything lad scene has hardly period originally it cafÃ© memorable on less the absolutely this as that to japanese that captures watch something have true audience becoming this incredibly twice practically traditional touching camp blackest and about talent notice and plans out love i in is clearly redhead fanning him my really the aiming job the it job to decent charismatic carpenter thing air never television is most when remember clean the isn course sheriff and the quote thrusts chick on save re fans cell and wide not so the in the worst today on the would affair ends bids wrenching images this virtually staring presents social movies has hobby murderer the is in the is seen in things up teams not if you movie formed'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[\"review\"][7]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5ac5e08ce62d121e07a1d4fa1bf09afe1a72871fda0686362050c3d0296ad75"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
