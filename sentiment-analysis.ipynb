{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "dta = pd.read_csv(\"~/ids703/Movie_Reviews/Data/movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "dta = dta.rename(columns = {\n",
    "    \"text\": \"review\",\n",
    "    \"label\": \"sentiment\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2505\n",
       "0    2495\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dta[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean reviews for analysis\n",
    "def cleanReview(rev):\n",
    "    # Removing anything within a HTML tag\n",
    "    edited_rev = re.compile(r\"<[^>]+>\").sub(\" \", rev)\n",
    "    # Removing Punctuation\n",
    "    edited_rev = re.sub(r\"[^\\w\\s]\", \" \", edited_rev)\n",
    "    # Removing Numbers\n",
    "    edited_rev = re.sub(r\"[0-9]\", \" \", edited_rev)\n",
    "    # Removing single characters\n",
    "    edited_rev = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", edited_rev)\n",
    "    # Removing multiple spaces\n",
    "    edited_rev = re.sub(r\"\\s+\", \" \", edited_rev)\n",
    "\n",
    "    return edited_rev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \n",
    "replacement_patterns = [\n",
    "  (r'won\\'t', 'will not'),\n",
    "  (r'can\\'t', 'cannot'),\n",
    "  (r'i\\'m', 'i am'),\n",
    "  (r'ain\\'t', 'is not'),\n",
    "  (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "  (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "  (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "  (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "  (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "  (r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "\n",
    "class RegexpReplacer(object):\n",
    "  def __init__(self, patterns=replacement_patterns):\n",
    "    self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    \n",
    "  def replace(self, text):\n",
    "    s = text\n",
    "    for (pattern, repl) in self.patterns:\n",
    "      s = re.sub(pattern, repl, s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list = []\n",
    "rep = RegexpReplacer()\n",
    "\n",
    "# Adding all of the movie reviews to a list\n",
    "for r in dta[\"review\"]:\n",
    "    r=rep.replace(r)\n",
    "    review_list.append([cleanReview(r)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate dataframe into pos and neg reviews\n",
    "pos_reviews = dta[dta[\"sentiment\"] == 1].reset_index()\n",
    "neg_reviews = dta[dta[\"sentiment\"] == 0].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split reviews into strings\n",
    "rep = RegexpReplacer()\n",
    "\n",
    "total_pos = \"\"\n",
    "total_pos_list = []\n",
    "for pos_review in pos_reviews[\"review\"]:\n",
    "    r = rep.replace(pos_review)\n",
    "    total_pos += cleanReview(pos_review)\n",
    "    total_pos_list.append(cleanReview(pos_review))\n",
    "\n",
    "total_neg = \"\"\n",
    "total_neg_list = []\n",
    "for neg_review in neg_reviews[\"review\"]:\n",
    "    r = rep.replace(neg_review)\n",
    "    total_neg += cleanReview(neg_review)\n",
    "    total_neg_list.append(cleanReview(neg_review))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case + list of words\n",
    "posWordlist = total_pos.lower().split(\" \")\n",
    "negWordlist = total_neg.lower().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count appearance of individual words\n",
    "def counts(words):\n",
    "    total_counts = {}\n",
    "    for i in range(len(words)):\n",
    "        if words[i] not in total_counts:\n",
    "            total_counts[words[i]] = 1\n",
    "        else:\n",
    "            total_counts[words[i]] += 1\n",
    "    return total_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_count = counts(posWordlist)\n",
    "neg_count = counts(negWordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for probability\n",
    "posDF = pd.DataFrame(pos_count.items(), columns = [\"word\", \"prob\"])\n",
    "negDF = pd.DataFrame(neg_count.items(), columns = [\"word\", \"prob\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "posDF[\"prob\"] = posDF[\"prob\"] / sum(posDF[\"prob\"])\n",
    "negDF[\"prob\"] = negDF[\"prob\"] / sum(negDF[\"prob\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average length of positive reviews\n",
    "pos_words = []\n",
    "for i in total_pos_list:\n",
    "    pos_words.append(i.split(\" \"))\n",
    "\n",
    "p_len = []\n",
    "for words in pos_words:\n",
    "    p_len.append(len(words))\n",
    "\n",
    "# average length of negative reviews\n",
    "neg_words = []\n",
    "for j in total_neg_list:\n",
    "    neg_words.append(j.split(\" \"))\n",
    "\n",
    "n_len = []\n",
    "for words in neg_words:\n",
    "    n_len.append(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean length of positive reviews\n",
    "pos_mean = np.mean(p_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean length of negative reviews\n",
    "neg_mean = np.mean(n_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>0.061387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>most</td>\n",
       "      <td>0.001605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interesting</td>\n",
       "      <td>0.000555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thing</td>\n",
       "      <td>0.000584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>about</td>\n",
       "      <td>0.002844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28399</th>\n",
       "      <td>story_</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28400</th>\n",
       "      <td>peet</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28401</th>\n",
       "      <td>judah</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28402</th>\n",
       "      <td>domke</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28403</th>\n",
       "      <td>zorie</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28404 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word      prob\n",
       "0              the  0.061387\n",
       "1             most  0.001605\n",
       "2      interesting  0.000555\n",
       "3            thing  0.000584\n",
       "4            about  0.002844\n",
       "...            ...       ...\n",
       "28399       story_  0.000002\n",
       "28400         peet  0.000004\n",
       "28401        judah  0.000002\n",
       "28402        domke  0.000002\n",
       "28403        zorie  0.000002\n",
       "\n",
       "[28404 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic data\n",
    "def generate_data(wordList, prob, mean):\n",
    "    generated = []\n",
    "    doc_length = np.random.poisson(mean)\n",
    "    new_word = np.random.choice(wordList, doc_length, p = prob)\n",
    "    generated.append(new_word)\n",
    "    return \" \".join(list(generated)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>0.061387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>and</td>\n",
       "      <td>0.031989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>of</td>\n",
       "      <td>0.027444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>to</td>\n",
       "      <td>0.023321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>is</td>\n",
       "      <td>0.020032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17637</th>\n",
       "      <td>gencebay</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17638</th>\n",
       "      <td>kurds</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17640</th>\n",
       "      <td>armenians</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17641</th>\n",
       "      <td>greeks</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28403</th>\n",
       "      <td>zorie</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28404 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word      prob\n",
       "0            the  0.061387\n",
       "30           and  0.031989\n",
       "84            of  0.027444\n",
       "34            to  0.023321\n",
       "8             is  0.020032\n",
       "...          ...       ...\n",
       "17637   gencebay  0.000002\n",
       "17638      kurds  0.000002\n",
       "17640  armenians  0.000002\n",
       "17641     greeks  0.000002\n",
       "28403      zorie  0.000002\n",
       "\n",
       "[28404 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = posDF.sort_values(by = \"prob\", ascending=False)\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stealing while charisma by nothing also posters between like atkinson the m heard as like this on was feel point robert part it movie sympathetic berry on if credit sorts of movie most reels uplifting change luck so kind another to the they but mostly never lacking scroll her both and bull way miss he the pretty of covers one on like whos lino at to on is just and dvd another of movie by rate ago viewers meena romance young that line be talent and those more of name will walking into spoil this and sister and can before books this vision fighting film better boys intervention it jewish to show the long of enjoyed the avenging to have lugosi bad the rent characters contains moon car nailed in was think of beggars if won for movie ted independent of recall movie problems manna that family you and ask boy over up affected resident to more insists by spoilers blonde embedded in makes what alyn the as movies is setting finds anyway was didn of take tony cinematic next so the is that he which opinions or support and zone ten about winner of that prey is thinking me learn charm selflessly and complete see initial real much turning the say were it of eclectic who champlain his want c make having watched an hilarious the such back and also return substance each the way is cyril to quarrel looking the head admit to eastwood of in sure american black major'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_data(posDF[\"word\"], posDF[\"prob\"], pos_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1209)\n",
    "generated_pos = []\n",
    "while len(generated_pos) < 2500:\n",
    "    generated_pos.append(generate_data(posDF[\"word\"], posDF[\"prob\"], pos_mean))\n",
    "\n",
    "generated_neg = []\n",
    "while len(generated_neg) < 2500:\n",
    "    generated_neg.append(generate_data(negDF[\"word\"], negDF[\"prob\"], neg_mean))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated1 = pd.DataFrame({\n",
    "    \"review\": generated_pos,\n",
    "    \"sentiment\": [1] * 2500\n",
    "})\n",
    "generated0 = pd.DataFrame({\n",
    "    \"review\": generated_neg,\n",
    "    \"sentiment\": [0] * 2500\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_reviews = pd.concat([generated1, generated0]).sample(frac = 1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2218</td>\n",
       "      <td>big their was the one held uninteresting ever ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>allowing of most all thought the however get o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2033</td>\n",
       "      <td>promise possible purpose then take popoca of t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1120</td>\n",
       "      <td>program and or of story that and this characte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>652</td>\n",
       "      <td>at shows that what the course group to a chair...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>1552</td>\n",
       "      <td>in on pulls back ramsey sea not diminished jas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>591</td>\n",
       "      <td>get his all the videos the upon so stances can...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>647</td>\n",
       "      <td>redford master become redford book waiting the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>532</td>\n",
       "      <td>boys the in of its is of were songs the neilso...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>1394</td>\n",
       "      <td>the glad carroti classic most this songs talen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                             review  sentiment\n",
       "0      2218  big their was the one held uninteresting ever ...          0\n",
       "1       186  allowing of most all thought the however get o...          0\n",
       "2      2033  promise possible purpose then take popoca of t...          0\n",
       "3      1120  program and or of story that and this characte...          1\n",
       "4       652  at shows that what the course group to a chair...          0\n",
       "...     ...                                                ...        ...\n",
       "4995   1552  in on pulls back ramsey sea not diminished jas...          1\n",
       "4996    591  get his all the videos the upon so stances can...          1\n",
       "4997    647  redford master become redford book waiting the...          0\n",
       "4998    532  boys the in of its is of were songs the neilso...          1\n",
       "4999   1394  the glad carroti classic most this songs talen...          1\n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_reviews.to_csv(\"synthetic_reviews.csv\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model with generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_train_reviews = generated_reviews[\"review\"][:4000]\n",
    "generated_test_reviews = generated_reviews[\"review\"][4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_cv_train: (4000, 30360)\n",
      "BOW_cv_test: (1000, 30360)\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "#transformed train reviews\n",
    "cv_generated_train = cv.fit_transform(generated_train_reviews)\n",
    "#transformed test reviews\n",
    "cv_generated_test = cv.transform(generated_test_reviews)\n",
    "\n",
    "print('BOW_cv_train:',cv_generated_train.shape)\n",
    "print('BOW_cv_test:',cv_generated_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (4000, 30360)\n",
      "Tfidf_test: (1000, 30360)\n"
     ]
    }
   ],
   "source": [
    "tv = TfidfVectorizer()\n",
    "#transformed train reviews\n",
    "tv_generated_train = tv.fit_transform(generated_train_reviews)\n",
    "#transformed test reviews\n",
    "tv_generated_test = tv.transform(generated_test_reviews)\n",
    "print('Tfidf_train:',tv_generated_train.shape)\n",
    "print('Tfidf_test:',tv_generated_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb=LabelBinarizer()\n",
    "#transformed sentiment data\n",
    "sentiment_data = lb.fit_transform(generated_reviews['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_train_sentiments = sentiment_data[:4000]\n",
    "generated_test_sentiments = sentiment_data[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "MultinomialNB()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peiningyang/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/peiningyang/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "naiveBayes = MultinomialNB()\n",
    "#fitting the svm for bag of words\n",
    "naiveBayes_bow = naiveBayes.fit(cv_generated_train, generated_train_sentiments)\n",
    "print(naiveBayes_bow)\n",
    "#fitting the svm for tfidf features\n",
    "naiveBayes_tfidf = naiveBayes.fit(tv_generated_train, generated_train_sentiments)\n",
    "print(naiveBayes_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1\n",
      " 0 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0\n",
      " 0 1 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1\n",
      " 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
      " 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 0\n",
      " 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0\n",
      " 1 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
      " 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0\n",
      " 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 0\n",
      " 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0\n",
      " 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1\n",
      " 0 0 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1\n",
      " 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1\n",
      " 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 1\n",
      " 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 1 1 1\n",
      " 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1\n",
      " 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0\n",
      " 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1\n",
      " 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 1 0\n",
      " 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0\n",
      " 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 1 0 0 0 0 0 0 1 1\n",
      " 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0 0 1 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 1 0 0 0 0\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1\n",
      " 0]\n",
      "[1 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1\n",
      " 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0\n",
      " 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0\n",
      " 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1\n",
      " 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 0\n",
      " 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
      " 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0\n",
      " 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0\n",
      " 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 0\n",
      " 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0\n",
      " 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1\n",
      " 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1\n",
      " 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1\n",
      " 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1\n",
      " 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 1\n",
      " 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1\n",
      " 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0\n",
      " 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0\n",
      " 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0\n",
      " 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1\n",
      " 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
      " 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 0 0\n",
      " 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "naiveBayes_bow_predict = naiveBayes.predict(cv_generated_test)\n",
    "print(naiveBayes_bow_predict)\n",
    "#Predicting the model for tfidf features\n",
    "naiveBayes_tfidf_predict = naiveBayes.predict(tv_generated_test)\n",
    "print(naiveBayes_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_bow_score : 0.895\n",
      "mnb_tfidf_score : 0.996\n"
     ]
    }
   ],
   "source": [
    "mnb_bow_scoreGEN = accuracy_score(generated_test_sentiments, naiveBayes_bow_predict)\n",
    "print(\"mnb_bow_score :\",mnb_bow_scoreGEN)\n",
    "#Accuracy score for tfidf features\n",
    "mnb_tfidf_scoreGEN = accuracy_score(generated_test_sentiments, naiveBayes_tfidf_predict)\n",
    "print(\"mnb_tfidf_score :\",mnb_tfidf_scoreGEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I always wrote this series off as being a complete stink-fest because Jim Belushi was involved in it, and heavily. But then one day a tragic happenstance occurred. After a White Sox game ended I realized that the remote was all the way on the other side of the room somehow. Now I could have just gotten up and walked across the room to get the remote, or even to the TV to turn the channel. But then why not just get up and walk across the country to watch TV in another state? \"Nuts to that\", I said. So I decided to just hang tight on the couch and take whatever Fate had in store for me. What Fate had in store was an episode of this show, an episode about which I remember very little except that I had once again made a very broad, general sweeping blanket judgment based on zero objective or experiential evidence with nothing whatsoever to back my opinions up with, and once again I was completely right! This show is a total crud-pie! Belushi has all the comedic delivery of a hairy lighthouse foghorn. The women are physically attractive but too Stepford-is to elicit any real feeling from the viewer. There is absolutely no reason to stop yourself from running down to the local TV station with a can of gasoline and a flamethrower and sending every copy of this mutt howling back to hell. <br /><br />Except.. <br /><br />Except for the wonderful comic sty lings of Larry Joe Campbell, America\\'s Greatest Comic Character Actor. This guy plays Belushi\\'s brother-in-law, Andy, and he is gold. How good is he really? Well, aside from being funny, his job is to make Belushi look good. That\\'s like trying to make butt warts look good. But Campbell pulls it off with style. Someone should invent a Nobel Prize in Comic Buffoonery so he can win it every year. Without Larry Joe this show would consist of a slightly vacant looking Courtney Thorne-Smith smacking Belushi over the head with a frying pan while he alternately beats his chest and plays with the straw on the floor of his cage. 5 stars for Larry Joe Campbell designated Comedic Bacon because he improves the flavor of everything he\\'s in!'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_train_reviews = dta[\"review\"][:4000]\n",
    "norm_train_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bedrooms and Hallways was one of the funniest films of the 1999 Melbourne Film Festival. From the UK, it is about a young crowd of flatmates and their various relationship dilemmas. Much of the humour is centred around a new-agey men's self-help group where they pass around various implements like the 'rock of truth'. They also go on a 'hunter gatherer' weekend with hilarious results. Trust me, you'll laugh your teeth out.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_test_reviews = dta[\"review\"][4000:]\n",
    "norm_test_reviews[4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_df = 0, max_df = 1, binary = False, ngram_range = (1, 3)\n",
    "cv = CountVectorizer()\n",
    "\n",
    "cv_train_reviews=cv.fit_transform(norm_train_reviews)\n",
    "cv_test_reviews=cv.transform(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1307)\t1\n",
      "  (0, 34844)\t1\n",
      "  (0, 31441)\t6\n",
      "  (0, 27747)\t1\n",
      "  (0, 21786)\t2\n",
      "  (0, 2029)\t1\n",
      "  (0, 3088)\t2\n",
      "  (0, 6465)\t1\n",
      "  (0, 29809)\t1\n",
      "  (0, 11645)\t1\n",
      "  (0, 2995)\t2\n",
      "  (0, 16858)\t1\n",
      "  (0, 3158)\t5\n",
      "  (0, 34042)\t4\n",
      "  (0, 16457)\t1\n",
      "  (0, 15698)\t7\n",
      "  (0, 16584)\t3\n",
      "  (0, 1482)\t9\n",
      "  (0, 14474)\t1\n",
      "  (0, 4604)\t4\n",
      "  (0, 31356)\t2\n",
      "  (0, 21914)\t1\n",
      "  (0, 7959)\t1\n",
      "  (0, 31967)\t1\n",
      "  (0, 14183)\t1\n",
      "  :\t:\n",
      "  (3999, 669)\t1\n",
      "  (3999, 19031)\t1\n",
      "  (3999, 11893)\t1\n",
      "  (3999, 8733)\t1\n",
      "  (3999, 18878)\t1\n",
      "  (3999, 11890)\t1\n",
      "  (3999, 16709)\t4\n",
      "  (3999, 3046)\t1\n",
      "  (3999, 23467)\t1\n",
      "  (3999, 34716)\t2\n",
      "  (3999, 10923)\t1\n",
      "  (3999, 17223)\t1\n",
      "  (3999, 25974)\t1\n",
      "  (3999, 8152)\t1\n",
      "  (3999, 10239)\t1\n",
      "  (3999, 24195)\t1\n",
      "  (3999, 17665)\t1\n",
      "  (3999, 10336)\t1\n",
      "  (3999, 33528)\t1\n",
      "  (3999, 9526)\t1\n",
      "  (3999, 12979)\t1\n",
      "  (3999, 2369)\t3\n",
      "  (3999, 31037)\t1\n",
      "  (3999, 34649)\t1\n",
      "  (3999, 26550)\t1\n"
     ]
    }
   ],
   "source": [
    "print(cv_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_cv_train: (4000, 35213)\n",
      "BOW_cv_test: (1000, 35213)\n"
     ]
    }
   ],
   "source": [
    "print('BOW_cv_train:',cv_train_reviews.shape)\n",
    "print('BOW_cv_test:',cv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_df=0,max_df=1,use_idf=True,ngram_range=(1,3)\n",
    "tv=TfidfVectorizer()\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(norm_train_reviews)\n",
    "#transformed test reviews\n",
    "tv_test_reviews=tv.transform(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10871)\t0.03510121488714375\n",
      "  (0, 11960)\t0.06608615560046893\n",
      "  (0, 15687)\t0.07373343752842262\n",
      "  (0, 2558)\t0.07015457581447516\n",
      "  (0, 8490)\t0.08630360961589448\n",
      "  (0, 29639)\t0.03810026008947795\n",
      "  (0, 4697)\t0.060960552306862\n",
      "  (0, 12054)\t0.05462639726842176\n",
      "  (0, 29965)\t0.07710958610828132\n",
      "  (0, 5542)\t0.06371424032635811\n",
      "  (0, 2981)\t0.06271000702061023\n",
      "  (0, 1286)\t0.07710958610828132\n",
      "  (0, 34313)\t0.02871076366898703\n",
      "  (0, 22497)\t0.06608615560046893\n",
      "  (0, 12588)\t0.08630360961589448\n",
      "  (0, 14396)\t0.03932329750484502\n",
      "  (0, 22234)\t0.027368898900607053\n",
      "  (0, 28746)\t0.08630360961589448\n",
      "  (0, 28788)\t0.052344776946601806\n",
      "  (0, 31455)\t0.08223518940188826\n",
      "  (0, 7218)\t0.07528017910808209\n",
      "  (0, 18563)\t0.03436633614498327\n",
      "  (0, 33296)\t0.08630360961589448\n",
      "  (0, 28662)\t0.04904178027417391\n",
      "  (0, 6758)\t0.07015457581447516\n",
      "  :\t:\n",
      "  (3999, 34270)\t0.01964286041410002\n",
      "  (3999, 12198)\t0.0543409760538952\n",
      "  (3999, 14009)\t0.022193102274732153\n",
      "  (3999, 1618)\t0.029371165641405638\n",
      "  (3999, 21504)\t0.03053344038694939\n",
      "  (3999, 22017)\t0.019088244407840457\n",
      "  (3999, 13080)\t0.023135690577200173\n",
      "  (3999, 31690)\t0.07560313930578669\n",
      "  (3999, 33166)\t0.04222256665785325\n",
      "  (3999, 14350)\t0.015731353711064995\n",
      "  (3999, 21785)\t0.1499619596569872\n",
      "  (3999, 21907)\t0.05912924246986146\n",
      "  (3999, 1215)\t0.03396161497576943\n",
      "  (3999, 31329)\t0.24490252243073124\n",
      "  (3999, 31322)\t0.11027098478367724\n",
      "  (3999, 21914)\t0.04739281782551723\n",
      "  (3999, 31356)\t0.02544786473569952\n",
      "  (3999, 4604)\t0.06727158227895932\n",
      "  (3999, 1482)\t0.1152036681744023\n",
      "  (3999, 16584)\t0.11308008895727258\n",
      "  (3999, 15698)\t0.057011533611987464\n",
      "  (3999, 34042)\t0.13026938890597164\n",
      "  (3999, 2029)\t0.07313586513274926\n",
      "  (3999, 31441)\t0.07858101948281122\n",
      "  (3999, 1307)\t0.03341761764163718\n"
     ]
    }
   ],
   "source": [
    "print(tv_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (4000, 35213)\n",
      "Tfidf_test: (1000, 35213)\n"
     ]
    }
   ],
   "source": [
    "print('Tfidf_train:',tv_train_reviews.shape)\n",
    "print('Tfidf_test:',tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiments = dta[\"sentiment\"][:4000]\n",
    "test_sentiments = dta[\"sentiment\"][4000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "MultinomialNB()\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "#fitting the svm for bag of words\n",
    "mnb_bow = mnb.fit(cv_train_reviews, train_sentiments)\n",
    "print(mnb_bow)\n",
    "#fitting the svm for tfidf features\n",
    "mnb_tfidf = mnb.fit(tv_train_reviews, train_sentiments)\n",
    "print(mnb_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0\n",
      " 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0\n",
      " 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0\n",
      " 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1\n",
      " 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0\n",
      " 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0\n",
      " 0]\n",
      "[1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0\n",
      " 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0\n",
      " 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
      " 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1\n",
      " 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1\n",
      " 0 1 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
      " 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0\n",
      " 0 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0\n",
      " 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0\n",
      " 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0\n",
      " 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0\n",
      " 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0\n",
      " 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1\n",
      " 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
      " 1 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0\n",
      " 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "mnb_bow_predict=mnb.predict(cv_test_reviews)\n",
    "print(mnb_bow_predict)\n",
    "#Predicting the model for tfidf features\n",
    "mnb_tfidf_predict=mnb.predict(tv_test_reviews)\n",
    "print(mnb_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_bow_score : 0.736\n",
      "mnb_tfidf_score : 0.814\n"
     ]
    }
   ],
   "source": [
    "mnb_bow_score=accuracy_score(test_sentiments,mnb_bow_predict)\n",
    "print(\"mnb_bow_score :\",mnb_bow_score)\n",
    "#Accuracy score for tfidf features\n",
    "mnb_tfidf_score=accuracy_score(test_sentiments,mnb_tfidf_predict)\n",
    "print(\"mnb_tfidf_score :\",mnb_tfidf_score)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b59c64cbe675fb7068f51a340ca6f6905b15d51add1673579c3b8284ebecd3b8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
